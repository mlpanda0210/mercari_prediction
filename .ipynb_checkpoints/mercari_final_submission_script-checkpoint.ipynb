{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It's ugly but it trains models first and then predict ;-)\n",
    "cat_name changed to 1, 1 since 1, 2 does not pass yet\n",
    "\"\"\"\n",
    "# import wordbatch\n",
    "# from wordbatch.extractors import WordBag, WordHash\n",
    "# from wordbatch.models import FTRL, FM_FTRL\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os, psutil\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import Ridge, SGDRegressor\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from multiprocessing import Process, Pool, Queue, JoinableQueue\n",
    "import functools\n",
    "from scipy.special import erfinv\n",
    "import re\n",
    "import unidecode\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import warnings\n",
    "import math\n",
    "from threading import Thread\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "MAX_FEATURES_ITEM_DESCRIPTION = 5000\n",
    "\n",
    "Hash_binary = True\n",
    "ensemble = True\n",
    "OOF = True\n",
    "\n",
    "PROD = \"production\"\n",
    "PROD_OOF = \"production_with_oof\"\n",
    "VALID_TRN = \"train_only_validation\"\n",
    "FAST_VALID = \"fast_validation_set\"\n",
    "STAGE2_OOF = \"stage2_with_OOF_validation\"\n",
    "STAGE2_PROD = \"Complete_Stage2_rehearsal\"\n",
    "\n",
    "non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n",
    "\n",
    "#####################################################################\n",
    "# Multi processing classes\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "class BaseWorker(Process):\n",
    "\n",
    "    def __init__(self, q_in, q_out):\n",
    "        self.task_queue: Queue = q_in\n",
    "        self.result_queue: Queue = q_out\n",
    "        super(BaseWorker, self).__init__()\n",
    "\n",
    "    def check_mem(self, dsp=\"\"):\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30\n",
    "        print(\"%s MEMORY USAGE for PID %5d : %.3f\" % (dsp, pid, memoryUse))\n",
    "\n",
    "\n",
    "class HashingWorker(BaseWorker):\n",
    "\n",
    "    def run(self):\n",
    "        # Get what's on the queue\n",
    "        # The queue should contain :\n",
    "        # an id for ordering purposes\n",
    "        # a data set\n",
    "        # a vectorizer that implements fit_transform\n",
    "        pid = os.getpid()\n",
    "        while True:\n",
    "            # self.check_mem(\"Hashing Worker\")\n",
    "            id_, data_, hashing_vectorizer_ = self.task_queue.get(block=True)\n",
    "            new_data = hashing_vectorizer_.fit_transform(data_)\n",
    "            self.task_queue.task_done()\n",
    "            self.result_queue.put([id_, new_data])\n",
    "            del new_data, data_, hashing_vectorizer_, id_\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "class ApplySeriesWorker(BaseWorker):\n",
    "    def run(self):\n",
    "        # Get what's on the queue\n",
    "        # The queue should contain :\n",
    "        # an id for ordering purposes\n",
    "        # a pd.Series\n",
    "        # a function to apply to the Series\n",
    "        pid = os.getpid()\n",
    "        while True:\n",
    "            # self.check_mem(\"Apply Series Worker\")\n",
    "            id_, data_, func_ = self.task_queue.get(block=True)\n",
    "            new_data = data_.apply(func_)\n",
    "            self.task_queue.task_done()\n",
    "            self.result_queue.put([id_, new_data])\n",
    "            del new_data, data_, func_, id_\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "class ApplyWorker(BaseWorker):\n",
    "    def run(self):\n",
    "        # Get what's on the queue\n",
    "        # The queue should contain\n",
    "        # an id for ordering purposes\n",
    "        # a pd.DataFrame to apply a function on\n",
    "        # a function to be applied on the pd.DataFrame\n",
    "        # the axis on which the function has to be applied (usually 1)\n",
    "        # a Boolean to say if data is passed as raw data to the function\n",
    "        pid = os.getpid()\n",
    "        while True:\n",
    "            # self.check_mem(\"Apply Worker\")\n",
    "            id_, data_, func_, axis_, raw_ = self.task_queue.get(block=True)\n",
    "            new_data = data_.apply(func_, axis=axis_, raw=raw_)\n",
    "            self.task_queue.task_done()\n",
    "            self.result_queue.put([id_, new_data])\n",
    "            del new_data, data_, func_, id_\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "class HashingManager(object):\n",
    "    def __init__(self, nb_workers=1):\n",
    "        self.q_jobs = JoinableQueue()\n",
    "        self.q_results = Queue()\n",
    "        self.nb_workers = nb_workers\n",
    "\n",
    "        self.hashing_workers = [HashingWorker(q_in=self.q_jobs, q_out=self.q_results)\n",
    "                                for _ in range(self.nb_workers)]\n",
    "\n",
    "        for wk in self.hashing_workers:\n",
    "            wk.start()\n",
    "\n",
    "    def apply(self, data_, hashing_vectorizer_):\n",
    "        # Split data in chuncks and put on queue\n",
    "        for id_, chunk_ in enumerate(np.array_split(data_, self.nb_workers)):\n",
    "            self.q_jobs.put([id_, chunk_, hashing_vectorizer_])\n",
    "            del id_, chunk_\n",
    "            gc.collect()\n",
    "\n",
    "        # Wait for tasks to complete\n",
    "        # Useless to wait, the get statement will do this\n",
    "        # plus the join will create a deadlock... stupid me\n",
    "        # self.q_jobs.join()\n",
    "\n",
    "        data_list = []\n",
    "        for i in range(self.nb_workers):\n",
    "            id, result = self.q_results.get()\n",
    "            data_list.append([id, result])\n",
    "            del id, result\n",
    "            gc.collect()\n",
    "\n",
    "        the_result = vstack([data_ for id_, data_ in sorted(data_list, key=lambda x: x[0])]).tocsr()\n",
    "\n",
    "        del data_list\n",
    "        gc.collect()\n",
    "\n",
    "        return the_result\n",
    "\n",
    "    def __del__(self):\n",
    "        for wk in self.hashing_workers:\n",
    "            wk.terminate()\n",
    "\n",
    "\n",
    "class ApplySeriesManager(object):\n",
    "    def __init__(self, nb_workers=1):\n",
    "        self.q_jobs = JoinableQueue()\n",
    "        self.q_results = Queue()\n",
    "        self.nb_workers = nb_workers\n",
    "        self.apply_series_workers = [ApplySeriesWorker(self.q_jobs, self.q_results) for _ in range(self.nb_workers)]\n",
    "\n",
    "        for wk in self.apply_series_workers:\n",
    "            wk.start()\n",
    "\n",
    "    def apply(self, data_, func_):\n",
    "        # Split data in chuncks and put on queue\n",
    "        for id_, chunk_ in enumerate(np.array_split(data_, self.nb_workers)):\n",
    "            self.q_jobs.put([id_, chunk_, func_])\n",
    "            del id_, chunk_\n",
    "            gc.collect()\n",
    "\n",
    "        # Wait for tasks to complete\n",
    "        # self.q_jobs.join()\n",
    "\n",
    "        data_list = []\n",
    "        for i in range(self.nb_workers):\n",
    "            id, result = self.q_results.get()\n",
    "            data_list.append([id, result])\n",
    "            del id, result\n",
    "            gc.collect()\n",
    "\n",
    "        the_result = pd.concat([data_ for id_, data_ in sorted(data_list, key=lambda x: x[0])],\n",
    "                               axis=0,\n",
    "                               ignore_index=True)\n",
    "\n",
    "        del data_list\n",
    "        gc.collect()\n",
    "\n",
    "        return the_result\n",
    "\n",
    "    def __del__(self):\n",
    "        for wk in self.apply_series_workers:\n",
    "            wk.terminate()\n",
    "\n",
    "\n",
    "class ApplyManager(object):\n",
    "    def __init__(self, nb_workers=1):\n",
    "        self.q_jobs = JoinableQueue()\n",
    "        self.q_results = Queue()\n",
    "        self.nb_workers = nb_workers\n",
    "        self.apply_workers = [ApplyWorker(self.q_jobs, self.q_results) for _ in range(self.nb_workers)]\n",
    "\n",
    "        for wk in self.apply_workers:\n",
    "            wk.start()\n",
    "\n",
    "    def apply(self, df=None, func=None, axis=0, raw=True):\n",
    "        # Split data in chuncks and put on queue\n",
    "        for id_, chunk_ in enumerate(np.array_split(df, self.nb_workers)):\n",
    "            self.q_jobs.put([id_, chunk_, func, axis, raw])\n",
    "            del id_, chunk_\n",
    "            gc.collect()\n",
    "\n",
    "        # Wait for tasks to complete\n",
    "        # self.q_jobs.join()\n",
    "\n",
    "        data_list = []\n",
    "        for i in range(self.nb_workers):\n",
    "            id, result = self.q_results.get()\n",
    "            data_list.append([id, result])\n",
    "            del id, result\n",
    "            gc.collect()\n",
    "\n",
    "        the_result = pd.concat([data_ for id_, data_ in sorted(data_list, key=lambda x: x[0])],\n",
    "                               axis=0,\n",
    "                               ignore_index=True)\n",
    "\n",
    "        del data_list\n",
    "        gc.collect()\n",
    "\n",
    "        return the_result\n",
    "\n",
    "    def __del__(self):\n",
    "        for wk in self.apply_workers:\n",
    "            wk.terminate()\n",
    "\n",
    "\n",
    "def fit_sgd_models(csr_ridge_trn, folds, y):\n",
    "    # print(\"THREADING EXPERIMENT\")\n",
    "    sgd_list = []\n",
    "    for fold_n, (trn_idx, val_idx) in enumerate(folds.split(csr_ridge_trn)):\n",
    "        sgd_list.append((\n",
    "            \"liblinear_fold_\" + str(fold_n),\n",
    "            LinearSVR(C=0.025,\n",
    "                      dual=True,\n",
    "                      epsilon=0.0,\n",
    "                      fit_intercept=True,\n",
    "                      intercept_scaling=1.0,\n",
    "                      loss='squared_epsilon_insensitive',\n",
    "                      max_iter=50,\n",
    "                      random_state=0,\n",
    "                      tol=0.0001,\n",
    "                      verbose=0),\n",
    "            trn_idx,\n",
    "            val_idx\n",
    "        ))\n",
    "        sgd_list.append((\n",
    "            \"ridge_fold_\" + str(fold_n),\n",
    "            Ridge(solver=\"sag\",\n",
    "                  fit_intercept=True,\n",
    "                  alpha=0.5,\n",
    "                  tol=0.05,\n",
    "                  random_state=666,\n",
    "                  max_iter=100),\n",
    "            trn_idx,\n",
    "            val_idx\n",
    "        ))\n",
    "    model_list = []\n",
    "    for i in range(folds.n_splits):\n",
    "        # print(\"Create a thread for %s\" % sgd_list[i * 2][0])\n",
    "        th1 = FitterThread(\n",
    "            name=sgd_list[i * 2][0],\n",
    "            model=sgd_list[i * 2][1],\n",
    "            data=csr_ridge_trn[sgd_list[i * 2][2]],\n",
    "            target=y[sgd_list[i * 2][2]]\n",
    "        )\n",
    "        # print(\"Create a thread for %s\" % sgd_list[i * 2 + 1][0])\n",
    "        th2 = FitterThread(\n",
    "            name=sgd_list[i * 2 + 1][0],\n",
    "            model=sgd_list[i * 2 + 1][1],\n",
    "            data=csr_ridge_trn[sgd_list[i * 2 + 1][2]],\n",
    "            target=y[sgd_list[i * 2 + 1][2]]\n",
    "        )\n",
    "        th1.start()\n",
    "        th2.start()\n",
    "        th1.join()\n",
    "        th2.join()\n",
    "\n",
    "        # Check the model has been fitted\n",
    "        val_preds_1 = sgd_list[i * 2][1].predict(csr_ridge_trn[sgd_list[i * 2][3]])\n",
    "        val_preds_2 = sgd_list[i * 2 + 1][1].predict(csr_ridge_trn[sgd_list[i * 2 + 1][3]])\n",
    "\n",
    "        print(\"Validation score for %s = %.6f\"\n",
    "              % (sgd_list[i * 2][0], mean_squared_error(y[sgd_list[i * 2][3]], val_preds_1) ** .5))\n",
    "        print(\"Validation score for %s = %.6f\"\n",
    "              % (sgd_list[i * 2 + 1][0], mean_squared_error(y[sgd_list[i * 2 + 1][3]], val_preds_2) ** .5))\n",
    "        model_list.append(sgd_list[i * 2][1])\n",
    "        model_list.append(sgd_list[i * 2 + 1][1])\n",
    "\n",
    "    return model_list\n",
    "\n",
    "\n",
    "class FitterThread(Thread):\n",
    "    \"\"\" Thread that fits a model on given data \"\"\"\n",
    "\n",
    "    def __init__(self, name, model, data, target):\n",
    "        Thread.__init__(self)\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def run(self):\n",
    "        self.model.fit(self.data, self.target)\n",
    "\n",
    "#####################################################################\n",
    "# Text preprocessing\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    try:\n",
    "        text = unidecode.unidecode(text)\n",
    "        text = str(text).lower()\n",
    "    except:\n",
    "        text=\"missing\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('❌', ' <HECROSS_MA ')\n",
    "    text = text.replace('⛔', ' <NO_ENT_EM')\n",
    "    text = text.replace('‼️', ' <HEAVY_EXCLAMATION> ')\n",
    "    text = text.replace('⭕', ' <HEAVY_LARGE_CIRCLE> ')\n",
    "    text = text.replace('❤️', ' <HEAVY_HEART_MARK> ')\n",
    "    text = text.replace('❗️', ' <HEAVY_EXCLAMATION_MARK ')\n",
    "    text = text.replace('✔', ' <HEAVY_CHECK_MARK> ')\n",
    "    text = text.replace('⭐️', ' <WHITE_MEDIUM_STAR_MARK> ')\n",
    "    text = text.replace('✅', ' <WHITE_HEAVY_CHECK_MARK> ')\n",
    "    text = text.replace('☺️', ' <SMILING_FACE_EMOJI> ')\n",
    "    text = text.replace('《', ' <DOUBLE_BRACKET_QUOTES> ')\n",
    "    text = text.replace('➡', ' <BLACK_RIGHTWARDS_ARROW> ')\n",
    "    text = text.replace('✴️', ' <EIGHT_POINTEDP_STAR> ')\n",
    "    # text = re.sub('\\&', \" and \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub('\\%', \" percent \", text, flags=re.IGNORECASE)\n",
    "    text = text.replace('.', ' <.> ')\n",
    "    text = text.replace(',', ' <,> ')\n",
    "    text = text.replace('，', ' <HEAVY_COMMA> ')\n",
    "    text = text.replace('\"', ' <\"> ')\n",
    "    text = text.replace(\"”\", ' <RIGHT_DBLE_QUOT> ')\n",
    "    text = text.replace(\"''\", ' <''> ')\n",
    "    text = text.replace('=', ' <=> ')\n",
    "    text = text.replace('+', ' <+> ')\n",
    "    text = text.replace('^^', ' <^^> ')\n",
    "    text = text.replace('^', ' <^> ')\n",
    "    text = text.replace('@', ' <@> ')\n",
    "    text = text.replace('*', ' <*> ')\n",
    "    text = text.replace(';', ' <;> ')\n",
    "    text = re.sub('\\$', \" dollar \", text, flags=re.IGNORECASE)\n",
    "    text = text.replace('!', ' <!> ')\n",
    "    text = text.replace('|', ' <|> ')\n",
    "    text = text.replace('∥', ' <PARALLEL_MARK> ')\n",
    "    text = text.replace('?', ' <?> ')\n",
    "    text = text.replace('~', ' <~> ')\n",
    "    text = text.replace('[', ' <[> ')\n",
    "    text = text.replace(']', ' <]> ')\n",
    "    text = text.replace('{', ' <{> ')\n",
    "    text = text.replace('}', ' <}> ')\n",
    "    text = text.replace('(', ' <(> ')\n",
    "    text = text.replace(')', ' <)> ')\n",
    "    text = text.replace('--', ' <--> ')\n",
    "    text = text.replace('-', ' <-> ')\n",
    "    # text = text.replace(\"\\\", ' <BLACKSLASH_MARK> ')\n",
    "    text = text.replace(\"/\", ' </> ')\n",
    "    # text = text.replace('[rm]', ' <REMOVED_PRICE> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <:> ')\n",
    "    text = text.replace('#', ' <#> ')\n",
    "    text = text.replace('gb', ' gb ')\n",
    "    text = text.replace('tb', ' tb ')\n",
    "    text = text.replace('karat', ' carat ')\n",
    "    text = text.replace('14k', ' 14carat ')\n",
    "    text = text.replace('14kt', ' 14carat ')\n",
    "    text = text.replace('18k', ' 18carat ')\n",
    "    text = text.replace('10k', ' 10carat ')\n",
    "    text = text.replace('nmd', ' nmds ')\n",
    "    text = text.replace('oz', ' oz ')\n",
    "    words = text.split()\n",
    "\n",
    "    words = ' '.join(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def name_preprocess(text):\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    try:\n",
    "        text = unidecode.unidecode(text)\n",
    "        text = str(text).lower()\n",
    "    except:\n",
    "        text = \"missing\"\n",
    "\n",
    "    text = text.replace('❌', ' <HEAVY_CROSS_')\n",
    "    text = text.replace('⭕', ' <HEAVY_LARGE_CIRCLE> ')\n",
    "    text = text.replace('⏳', ' <Hourglass_With_Flowing_Sand> ')\n",
    "\n",
    "    text = text.replace('♨', ' <HOT_SPRINGS> ')\n",
    "    text = text.replace('✌️️', ' <VICTORY_HAND> ')\n",
    "    text = text.replace('⛅', ' <SUN_BEHIND_CLOUD> ')\n",
    "    text = text.replace('♌', ' <LEO_MARK> ')\n",
    "    text = text.replace('☠', ' <SKULL_CROSSBONES> ')\n",
    "    text = text.replace('⬇', ' <DOWNWARDS_BLACK_ARROW> ')\n",
    "    text = text.replace('♠️️', ' <BLACK_SPADE> ')\n",
    "    text = text.replace('♤', ' <WHITE_SPADE> ')\n",
    "    text = text.replace('⚫️', ' <MEDIUM_BLACK_CIRCLE> ')\n",
    "    text = text.replace('⁉️', ' <EXCLAMATION_QUESTION_MARK> ')\n",
    "    text = text.replace('⛄', ' <SNOW_MAN_NO_SNOW> ')\n",
    "    text = text.replace('⚁', ' <DIE_FACE_EMOJI> ')\n",
    "    text = text.replace('✈️', ' <Airplane_Emoji> ')\n",
    "    text = text.replace('♢', ' <WHITE_DIAMON> ')\n",
    "    text = text.replace('➰', ' <Curly_Loop> ')\n",
    "    text = text.replace('➕', ' <HEAVY_PLUS_SIGN_EMOJI> ')\n",
    "    text = text.replace('✂', ' <Black_Scissors_EMOJI> ')\n",
    "    text = text.replace('❄️', ' <SNOWFLAKE_EMOJI> ')\n",
    "    text = text.replace('☒', ' <BALLOT_BOX> ')\n",
    "    text = text.replace('☘️', ' <SHAMROCK_EMOJI> ')\n",
    "    text = text.replace('⚠', ' <WARNING_SIGN_EMOJI> ')\n",
    "    text = text.replace('⚜', ' <Fleur_De_Lis_EMOJI> ')\n",
    "    text = text.replace('☮', ' <PEACE_SYMBOL> ')\n",
    "    text = text.replace('☄', ' <COMET_EMOJI> ')\n",
    "    text = text.replace('❣', ' <HEAVY_HEART_EXCLAMATION_Mark> ')\n",
    "    text = text.replace('❥', ' <ROTATE_HEAVY_BLACK_HEART_BULLET> ')\n",
    "    text = text.replace('✉️', ' <ENVELOPE_EMOJI> ')\n",
    "    text = text.replace('✖︎', ' <HEAVY_BLACK_CROSS> ')\n",
    "    text = text.replace('‼️', ' <HEAVY_EXCLAMATION> ')\n",
    "    text = text.replace('✮', ' <HEAVY_OUTLINED_BLACK_STAR> ')\n",
    "    text = text.replace('★', ' <HEAVY_OUTLINED_WHITE_STAR> ')\n",
    "    text = text.replace('⛔', ' <NO_ENTRY_EMOJI> ')\n",
    "    text = re.sub('⚡️', ' <HEAVY_VOLTAGE_SIGN_EMOJI> ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('⚡', ' <HEAVY_VOLTAGE_SIGN_EMOJI> ', text, flags=re.IGNORECASE)\n",
    "    text = text.replace('❤️', ' <HEAVY_HEART_MARK> ')\n",
    "    text = text.replace('☕️', ' <HOT_BEVERAGE_EMOJI> ')\n",
    "    text = text.replace('❗️', ' <HEAVY_EXCLAMATION_MARK> ')\n",
    "    text = text.replace('✔', ' <HEAVY_CHECK_MARK> ')\n",
    "    text = re.sub('⭐️', ' <WHITE_MEDIUM_STAR_MARK> ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('⭐', ' <WHITE_MEDIUM_STAR_MARK> ', text, flags=re.IGNORECASE)\n",
    "    text = text.replace('❤⭐', ' <HEART_WHITE_MEDIUM_STAR_MARK> ')\n",
    "    text = text.replace('❤', ' <HEAVY_HEART_MARK> ')\n",
    "    text = text.replace('▪️', ' <BLACK_SMALL_SQUARE> ')\n",
    "    text = text.replace('✅', ' <WHITE_HEAVY_CHECK_MARK> ')\n",
    "    text = text.replace('❎', ' <Negative_Squared_Cross_Mark> ')\n",
    "    text = text.replace('•', ' <BULLET_MARK> ')\n",
    "    text = text.replace('●', ' <HEAVY_BULLET_MARK> ')\n",
    "    text = text.replace('©', ' <COPY_RIGHT_SIGN> ')\n",
    "    text = text.replace('®', ' <REGISTERED_SIGN_MARK> ')\n",
    "    text = text.replace('♡', ' <HEART_SYMBOL> ')\n",
    "    text = text.replace('☆', ' <WHITE_STAR_SYMBOL> ')\n",
    "    text = text.replace('★', ' <BLACK_STAR_SYMBOL> ')\n",
    "    text = text.replace('✨', ' <SPARKLES_EMOJI ')\n",
    "    text = text.replace('☺️', ' <SMILING_FACE_EMOJI> ')\n",
    "    text = text.replace('《', ' <DOUBLE_LEFT_BRACKET_QUOTES> ')\n",
    "    text = text.replace('》', ' <DOUBLE_RIGHT_BRACKET_QUOTES> ')\n",
    "    text = text.replace('〰', ' <WAVY_DASH_EMOJI> ')\n",
    "    text = text.replace('➡', ' <BLACK_RIGHTWARDS_ARROW> ')\n",
    "    text = text.replace('✴️', ' <EIGHT_POINTED_STAR> ')\n",
    "    # text = re.sub('\\&', \" and \", text, flags=re.IGNORECASE)\n",
    "    # text = re.sub('\\%', \" percent \", text, flags=re.IGNORECASE)\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('，', ' <HEAVY_COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(\"”\", ' <RIGHT_DOUBLE_QUOTATION_MARK> ')\n",
    "    text = text.replace(\"''\", ' <DOUBLE_QUOTATION_MARK> ')\n",
    "    text = text.replace('=', ' <EQUAL_SIGN_MARK> ')\n",
    "    text = text.replace('+', ' <PLUSL_SIGN> ')\n",
    "    text = text.replace('^^', ' <_DOUBLE_CARET_MARK> ')\n",
    "    text = text.replace('^', ' <CARET_MARK> ')\n",
    "    text = text.replace('@', ' <AT_SIGN> ')\n",
    "    text = text.replace('*', ' <STAR_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    # text = re.sub('\\$', \" dollar \", text, flags=re.IGNORECASE)\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('|', ' <VERTICAL_BAR_MARK> ')\n",
    "    text = text.replace('∥', ' <PARALLEL_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('~', ' <TILDE_MARK> ')\n",
    "    text = text.replace('[', ' <LEFT_SQUARE_BRACKET> ')\n",
    "    text = text.replace(']', ' <RIGHT_SQUARE_BRACKET> ')\n",
    "    text = text.replace('{', ' <LEFT_CURLY_BRACKET> ')\n",
    "    text = text.replace('}', ' <RIGHT_CURLY_BRACKET> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('-', ' <HYPHENS> ')\n",
    "    # text = text.replace(\"\\\", ' <BLACKSLASH_MARK> ')\n",
    "    text = text.replace(\"/\", ' <SLASH_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    # text = text.replace('[rm]', ' <REMOVED_PRICE> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    text = text.replace('#', ' <HASH> ')\n",
    "    text = text.replace('gb', ' gb ')\n",
    "    text = text.replace('tb', ' tb ')\n",
    "    text = text.replace('karat', ' carat ')\n",
    "    text = text.replace('14k', ' 14carat ')\n",
    "    text = text.replace('14kt', ' 14carat ')\n",
    "    text = text.replace('18k', ' 18carat ')\n",
    "    text = text.replace('10k', ' 10carat ')\n",
    "    text = text.replace('nmd', ' nmds ')\n",
    "    text = text.replace('oz', ' oz ')\n",
    "    text = re.sub(\"\\'ve\", \" have \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"n't\", \" not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'d\", \" would \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'s\", \"\", text, flags=re.IGNORECASE)\n",
    "    words = text.split()\n",
    "    words = ' '.join(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def simple_preprocess(text):\n",
    "    \"\"\"Just making sure we have text\"\"\"\n",
    "    try:\n",
    "        text = unidecode.unidecode(text)\n",
    "        text = str(text).lower()\n",
    "    except:\n",
    "        text = \"missing\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_cond_id(z):\n",
    "    try:\n",
    "        if z > 5:\n",
    "            return 1\n",
    "        elif z < 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return z\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def process_shipping(z):\n",
    "    try:\n",
    "        if z not in [0, 1]:\n",
    "            return 0\n",
    "        else:\n",
    "            return z\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def simulate_test(test):\n",
    "    if test.shape[0] < 800000:\n",
    "        indices = np.random.choice(test.index.values, 2800000)\n",
    "        test_ = pd.concat([test, test.iloc[indices]], axis=0)\n",
    "        return test_.copy()\n",
    "    else:\n",
    "        return test\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return u\" \".join(\n",
    "        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")]])\n",
    "\n",
    "\n",
    "def cpuStats(disp=\"\"):\n",
    "    \"\"\" @author: RDizzl3 @address: https://www.kaggle.com/rdizzl3\"\"\"\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0] / 2. ** 30\n",
    "    print(\"%s MEMORY USAGE for PID %10d : %.3f\" % (disp, pid, memoryUse))\n",
    "\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['name'].fillna(value='missing', inplace=True)\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='No description yet', inplace=True)\n",
    "\n",
    "\n",
    "def mix_cat_name(row):\n",
    "    \"\"\"\n",
    "      Mixes words in name with categories in the category feature\n",
    "      categories are expected to be in the first field of row and separated by /\n",
    "      name is expected to be in the second field of row\n",
    "      \"\"\"\n",
    "    return \" \".join([category + \"_\" + word\n",
    "                     for category in row[0].lower().split(\"/\") for word in row[1].lower().split()])\n",
    "\n",
    "\n",
    "def mix_cat_name_cond(row):\n",
    "    \"\"\"\n",
    "      Mixes words in name with categories in the category feature and with the item condition\n",
    "      categories are expected to be in the first field of row and separated by /\n",
    "      name is expected to be in the second field of row\n",
    "      item condition is expected to be in the third field\n",
    "      \"\"\"\n",
    "    return \" \".join([category + \"_\" + word + \" \" + category + \"_\" + word + \"_\" + str(row[2])\n",
    "                     for category in row[0].lower().split(\"/\") for word in row[1].lower().split()])\n",
    "\n",
    "\n",
    "def mix_cat_cond(row):\n",
    "    \"\"\"\n",
    "      Mixes categories in the category feature with the item condition\n",
    "      categories are expected to be in the first field of row and separated by /\n",
    "      item condition is expected to be in the second field\n",
    "      \"\"\"\n",
    "    return \" \".join([category + \"_\" + str(row[2])\n",
    "                     for category in row[0].lower().split(\"/\")])\n",
    "\n",
    "\n",
    "def add_price_statistics_on_train(trn, target, feature=None):\n",
    "    \"\"\"\n",
    "      Prices are aggregated by brand to compute statistics\n",
    "      These stats are then merged back into train and test datasets\n",
    "      @author: Kueipo @address: https://www.kaggle.com/kueipo\n",
    "      @param: trn: training data taht is expected to contain a 'price' feature\n",
    "      @param: sub: test data\n",
    "      \"\"\"\n",
    "    #train[\"brand_name\"]をコピー\n",
    "    train = trn[[feature]].copy()\n",
    "    train[\"price\"] = target\n",
    "#train[\"brand_name\"]でnullのものを除外\n",
    "    train = train[train[feature].notnull()]\n",
    "    #groupbyによりそれぞれのブランドごとの価格の統計値を求める\n",
    "    stats = train.groupby(feature)['price'].agg({'median', \"mean\", 'std', 'min', 'max'}).reset_index()\n",
    "    #標準偏差が0のものに対しては0を入れる\n",
    "    stats[\"std\"].fillna(0, inplace=True)\n",
    "    #カラムの名称を変更する\n",
    "    stats.columns = [feature, feature + \"_median\", feature + \"_mean\",\n",
    "                     feature + \"_std\", feature + \"_min\", feature + \"_max\"]\n",
    "    #trainデータに対してブランドごとの統計値を入れる\n",
    "    trn = pd.merge(trn, stats, how='left', on=feature)\n",
    "\n",
    "    # Now set unknown values to the overall median, std, min or max\n",
    "    #null値に関してはすべてのデータの統計値を入れる\n",
    "    trn.loc[trn[feature + \"_median\"].isnull(), feature + \"_median\"] = np.median(target)\n",
    "    trn.loc[trn[feature + \"_mean\"].isnull(), feature + \"_mean\"] = np.mean(target)\n",
    "    trn.loc[trn[feature + \"_std\"].isnull(), feature + \"_std\"] = np.std(target)\n",
    "    trn.loc[trn[feature + \"_min\"].isnull(), feature + \"_min\"] = np.min(target)\n",
    "    trn.loc[trn[feature + \"_max\"].isnull(), feature + \"_max\"] = np.max(target)\n",
    "\n",
    "    del train\n",
    "    gc.collect()\n",
    "\n",
    "    return trn, stats\n",
    "\n",
    "\n",
    "def add_price_statistics_on_test(sub, stats, target, feature=None):\n",
    "    \"\"\"\n",
    "      Prices are aggregated by brand to compute statistics\n",
    "      These stats are then merged back into train and test datasets\n",
    "      @author: Kueipo @address: https://www.kaggle.com/kueipo\n",
    "      @param: trn: training data taht is expected to contain a 'price' feature\n",
    "      @param: sub: test data\n",
    "      \"\"\"\n",
    "    sub = pd.merge(sub, stats, how='left', on=feature)\n",
    "    # Now set unknown values to the overall median, std, min or max\n",
    "    sub.loc[sub[feature + \"_median\"].isnull(), feature + \"_median\"] = np.median(target)\n",
    "    sub.loc[sub[feature + \"_mean\"].isnull(), feature + \"_mean\"] = np.mean(target)\n",
    "    sub.loc[sub[feature + \"_std\"].isnull(), feature + \"_std\"] = np.std(target)\n",
    "    sub.loc[sub[feature + \"_min\"].isnull(), feature + \"_min\"] = np.min(target)\n",
    "    sub.loc[sub[feature + \"_max\"].isnull(), feature + \"_max\"] = np.max(target)\n",
    "\n",
    "    return sub\n",
    "\n",
    "\n",
    "def string_len(x):\n",
    "    \"\"\" Simple function that returns the len of a string \"\"\"\n",
    "    try:\n",
    "        return len(str(x))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def word_count(x, sep=None):\n",
    "    \"\"\" Simple function that returns the number of words in a string \"\"\"\n",
    "    try:\n",
    "        return len(str(x).split(sep))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def add_character_and_word_lengths(data, app_series_man_):\n",
    "    \"\"\"\n",
    "      @author: Olivier @address: https://www.kaggle.com/ogrellier\n",
    "      Function used to create additional features.\n",
    "      All of this is parallelized using process pooling\n",
    "      \"\"\"\n",
    "    # Apply description length in parallel\n",
    "    #item_descriptionの長さを入れる\n",
    "    data[\"desc_len\"] = app_series_man_.apply(data_=data[\"item_description\"].fillna(\"missing\"), func_=string_len)\n",
    "    data[\"desc_word_len\"] = app_series_man_.apply(data_=data[\"item_description\"].fillna(\"missing\"),\n",
    "                                                  func_=functools.partial(word_count, sep=None))\n",
    "    data[\"nb_categories\"] = app_series_man_.apply(data_=data[\"category_name\"].fillna(\"missing\"),\n",
    "                                                  func_=functools.partial(word_count, sep=\"/\"))\n",
    "    data[\"name_len\"] = app_series_man_.apply(data_=data[\"name\"].fillna(\"missing\"), func_=string_len)\n",
    "    data[\"name_word_len\"] = app_series_man_.apply(data_=data[\"name\"].fillna(\"missing\"),\n",
    "                                                  func_=functools.partial(word_count, sep=None))\n",
    "    # Add ratios\n",
    "    # data[\"ratio_1\"] = data[\"name_len\"] / (data[\"name_word_len\"] + 1)\n",
    "    # data[\"ratio_2\"] = data[\"desc_len\"] / (data[\"desc_word_len\"] + 1)\n",
    "    # data[\"ratio_3\"] = data[\"name_len\"] / (data[\"desc_len\"] + 1)\n",
    "    # data[\"ratio_4\"] = data[\"name_word_len\"] / (data[\"desc_word_len\"] + 1)\n",
    "\n",
    "\n",
    "def add_combination_category_name(data, app_man_):\n",
    "\n",
    "    data[\"mix_cat_name\"] = app_man_.apply(df=data[[\"category_name\", \"name\"]].fillna(\"missing\"),\n",
    "                                          func=mix_cat_name,\n",
    "                                          axis=1,\n",
    "                                          raw=True)\n",
    "\n",
    "\n",
    "\n",
    "def add_categories_and_mix_with_condition(df):\n",
    "    for i in range(3):\n",
    "        # Create new features\n",
    "        df[\"category_name_\" + str(i)] = df[\"category_name\"].str.split(\"/\").str[i].fillna(\"no_cat\")\n",
    "        df[\"cat_cond_\" + str(i)] = df[\"category_name_\" + str(i)] + '|' + df[\"item_condition_id\"].apply(lambda x: str(x))\n",
    "\n",
    "\n",
    "def preprocess_text_features(df, app_series_man_):\n",
    "    \"\"\"\n",
    "      Utility function to apply text pre-processing by Kueipo to name, brand and description\n",
    "      but in parallel\n",
    "      \"\"\"\n",
    "    df[\"item_description\"] = app_series_man_.apply(data_=df[\"item_description\"].fillna(\"missing\"), func_=preprocess)\n",
    "    df[\"name\"] = app_series_man_.apply(data_=df[\"name\"].fillna(\"missing\"), func_=preprocess)\n",
    "    df[\"brand_name\"] = app_series_man_.apply(data_=df[\"brand_name\"].fillna(\"missing\"), func_=simple_preprocess)\n",
    "    df[\"category_name\"] = app_series_man_.apply(data_=df[\"category_name\"].fillna(\"missing\"), func_=simple_preprocess)\n",
    "    df[\"shipping\"] = app_series_man_.apply(data_=df[\"shipping\"].fillna(-1), func_=process_shipping)\n",
    "    df[\"item_condition_id\"] = app_series_man_.apply(data_=df[\"item_condition_id\"].fillna(-1), func_=process_cond_id)\n",
    "\n",
    "\n",
    "def add_d(text):\n",
    "    \"\"\"\n",
    "      Simple text modification used on description to ensure\n",
    "      words in description are not hashed in the same space as name\n",
    "      \"\"\"\n",
    "    str = \" \".join([\"d_\" + w for w in TweetTokenizer().tokenize(text)])\n",
    "    print(str)\n",
    "    return str\n",
    "\n",
    "\n",
    "def add_b(text):\n",
    "    \"\"\"\n",
    "      Simple text modification used brand to ensure\n",
    "      brands are not hashed in the same space as name and description\n",
    "      \"\"\"\n",
    "    return \" \".join([\"b_\" + w for w in text.split()])\n",
    "\n",
    "\n",
    "# def get_hashing_features(train, test, Hash_binary, start_time):\n",
    "def get_hashing_features(df, hash_binary, start_time, app_series_man_, app_man_, hash_man_):\n",
    "    # df = pd.concat([train, test])\n",
    "    dim = 24\n",
    "    #nameはTweetTokenizerを使ってからベクトル化\n",
    "    cv_name = HashingVectorizer(\n",
    "        n_features=2 ** dim,\n",
    "        ngram_range=(1, 2),\n",
    "        norm=None,\n",
    "        alternate_sign=False,\n",
    "        tokenizer=TweetTokenizer().tokenize,\n",
    "        binary=hash_binary\n",
    "    )\n",
    "    X_name = hash_man_.apply(data_=df[\"name\"], hashing_vectorizer_=cv_name)\n",
    "\n",
    "\n",
    "    cv_cat_name = HashingVectorizer(\n",
    "        n_features=2 ** dim,\n",
    "        ngram_range=(1, 2),\n",
    "        norm=None,\n",
    "        alternate_sign=False,\n",
    "        tokenizer=None,  # TweetTokenizer().tokenize,\n",
    "        binary=hash_binary\n",
    "    )\n",
    "    #df[\"mix_cat_name\"]を作る。これはcagorynameとnameを掛け合わせたmatrix。それに対してhashing_vectorizserを使う\n",
    "    add_combination_category_name(df, app_man_)\n",
    "\n",
    "    X_name += hash_man_.apply(data_=df[\"mix_cat_name\"], hashing_vectorizer_=cv_cat_name)\n",
    "\n",
    "\n",
    "    desc_hash = HashingVectorizer(n_features=2 ** dim,\n",
    "                                  norm=None,\n",
    "                                  alternate_sign=False,\n",
    "                                  tokenizer=None,  # TweetTokenizer().tokenize,\n",
    "                                  binary=hash_binary\n",
    "                                  # stop_words='english'\n",
    "                                  )\n",
    "\n",
    "    #df[\"get_desc_hash_out\"]を作る。これはitem_descriptionの単語にd_を足したもの。それに対してhashing_vectorizserを使う\n",
    "    df[\"get_desc_hash_out\"] = app_series_man_.apply(data_=df[\"item_description\"].fillna(\"missing\"), func_=add_d)\n",
    "\n",
    "    X_name += hash_man_.apply(data_=df[\"get_desc_hash_out\"], hashing_vectorizer_=desc_hash)\n",
    "\n",
    "    df.drop(\"get_desc_hash_out\", axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "\n",
    "    #df[\"get_brand_hash_out\"]を作る。これはibrand_nameの単語にb_を足したもの。それに対してhashing_vectorizserを使う\n",
    "    df[\"get_brand_hash_out\"] = app_series_man_.apply(data_=df[\"brand_name\"].fillna(\"missing\"), func_=add_b)\n",
    "\n",
    "\n",
    "    brd_hash = HashingVectorizer(n_features=2 ** dim,\n",
    "                                 norm=None,\n",
    "                                 alternate_sign=False,\n",
    "                                 binary=hash_binary\n",
    "                                 )\n",
    "    X_name += hash_man_.apply(data_=df[\"get_brand_hash_out\"], hashing_vectorizer_=brd_hash)\n",
    "\n",
    "    #Hashing Vectrizerを使っているのはname,df[\"mix_cat_name\"],df[\"get_desc_hash_out\"],df[\"get_brand_hash_out\"]\n",
    "\n",
    "    df.drop(\"get_brand_hash_out\", axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    print('[{}] Finished hashing dataset'.format(time.time() - start_time))\n",
    "\n",
    "    return X_name\n",
    "\n",
    "\n",
    "def get_tfidf_features_for_train(train, hash_man_):\n",
    "    # Create wordbatch tfidf\n",
    "    wb = HashingVectorizer(\n",
    "        n_features=2 ** 20,\n",
    "        ngram_range=(1, 1),\n",
    "        norm=None,\n",
    "        alternate_sign=False,\n",
    "        tokenizer=TweetTokenizer().tokenize,\n",
    "        binary=Hash_binary\n",
    "    )\n",
    "    X_name = hash_man_.apply(data_=train[\"name\"], hashing_vectorizer_=wb)\n",
    "\n",
    "    # print(\"Wordbatch hashing done\")\n",
    "\n",
    "    # Remove features with document frequency <=1\n",
    "    # This is not a stateless step\n",
    "    # If clipping is an np.array it will take a massive amount of memory\n",
    "    # clipping = np.array(np.clip(X_name.getnnz(axis=0) - 1, 0, 1), dtype=bool)\n",
    "    clipping = np.array((X_name.sum(axis=0) >= 1))[0]\n",
    "    # cpuStats()\n",
    "    print(\"Clipping computed\")\n",
    "    X_name = X_name[:, clipping]\n",
    "    gc.collect()\n",
    "    # cpuStats()\n",
    "    print(\"X_name reduced\")\n",
    "    # return matrix and wordbatch for future use\n",
    "    return X_name, wb, clipping\n",
    "\n",
    "\n",
    "def get_tfidf_features_for_test(test, wb, clipping, hash_man_):\n",
    "\n",
    "    X_name = hash_man_.apply(data_=test[\"name\"], hashing_vectorizer_=wb)\n",
    "    X_name = X_name[:, clipping]\n",
    "\n",
    "    return X_name\n",
    "\n",
    "\n",
    "class OHEManager(object):\n",
    "\n",
    "    def __init__(self, feature_name=None, min_df=5):\n",
    "        self.name = feature_name\n",
    "        self.indexer = None\n",
    "        self.ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "        self.indices = None\n",
    "        self.cols = None\n",
    "        self.min_df = min_df\n",
    "\n",
    "    def add_factorized_feature_on_train(self, trn):\n",
    "        trn[\"fact_\" + self.name], self.indexer = pd.factorize(trn[self.name])\n",
    "\n",
    "    def add_factorized_feature_on_test(self, sub):\n",
    "        if self.indexer is None:\n",
    "            raise ValueError(\"indexer has not been fitted yet\")\n",
    "\n",
    "        sub[\"fact_\" + self.name] = self.indexer.get_indexer(sub[self.name])\n",
    "\n",
    "    def get_feature_for_sgd_train(self, trn):\n",
    "        dummies = self.ohe.fit_transform(trn[[\"fact_\" + self.name]].replace(-1, 999))\n",
    "        self.indices = np.arange(dummies.shape[1])\n",
    "        self.cols = np.array((dummies.sum(axis=0) >= self.min_df))[0]\n",
    "        return dummies[:, self.indices[self.cols]] \n",
    "\n",
    "    def get_feature_for_sgd_test(self, sub):\n",
    "        dummies = self.ohe.transform(sub[[\"fact_\" + self.name]].replace(-1, 999))\n",
    "        return dummies[:, self.indices[self.cols]]\n",
    "\n",
    "\n",
    "def get_numerical_features_for_sgd(df):\n",
    "    # Factors cannot be used by linear models\n",
    "    numericals = [\n",
    "        \"fact_category_name_0\", \"fact_category_name_1\", \"fact_category_name_2\",\n",
    "        \"fact_cat_cond_0\", \"fact_cat_cond_1\", \"fact_cat_cond_2\",\n",
    "        \"fact_brand_name\",\n",
    "        \"item_condition_id\",\n",
    "        \"shipping\",\n",
    "        \"desc_len\", \"desc_word_len\", \"name_len\", \"name_word_len\", \"nb_categories\",\n",
    "        \"brand_name_median\", \"brand_name_std\", \"brand_name_min\", \"brand_name_max\", \"distance\",\n",
    "        \"category_name_median\", \"category_name_std\", \"category_name_min\", \"category_name_max\",\n",
    "        \"ratio_1\", \"ratio_2\", \"ratio_3\", \"ratio_4\",\n",
    "    ]\n",
    "    return [f_ for f_ in numericals if f_ in df]\n",
    "\n",
    "\n",
    "def get_numerical_features_for_lgb(df):\n",
    "    numericals = [\n",
    "        \"fact_category_name_0\", \"fact_category_name_1\", \"fact_category_name_2\",\n",
    "        \"fact_cat_cond_0\", \"fact_cat_cond_1\", \"fact_cat_cond_2\",\n",
    "        \"fact_brand_name\",\n",
    "        \"item_condition_id\", \"shipping\",\n",
    "        \"desc_len\", \"desc_word_len\", \"name_len\", \"name_word_len\", \"nb_categories\",\n",
    "        \"brand_name_median\", \"brand_name_std\", \"brand_name_min\", \"brand_name_max\", \"distance\",\n",
    "        \"category_name_median\", \"category_name_std\", \"category_name_min\", \"category_name_max\",\n",
    "        \"sgd_liblinear\", \"sgd_ridge\", \"liblinear_ridge\",\n",
    "        \"fact_name_0\", \"fact_name_1\", \"fact_name_2\", \"fact_name_3\", \"fact_name_4\", \"fact_name_5\",\n",
    "        \"ratio_1\", \"ratio_2\", \"ratio_3\", \"ratio_4\",\n",
    "    ]\n",
    "\n",
    "    return [f_ for f_ in numericals if f_ in df]\n",
    "\n",
    "\n",
    "def get_numerical_features(df, numericals=None,\n",
    "                           gaussian=True, rank=False,\n",
    "                           minmax_skl=None):\n",
    "    num_feats = [f_ for f_ in numericals if f_ in df]\n",
    "    # print(num_feats)\n",
    "\n",
    "    if gaussian:\n",
    "        if rank:\n",
    "            num_df = df[num_feats].copy()\n",
    "            for f_ in num_feats:\n",
    "                num_df[f_] = (num_df[f_].rank() - num_df.shape[0] * .5) / (num_df.shape[0] * .5)\n",
    "            num_df[num_df >= 1.0] = 0.99999\n",
    "            num_df[num_df <= -1.0] = -0.99999\n",
    "        else:\n",
    "            if minmax_skl is None:\n",
    "                minmax_skl = MinMaxScaler(feature_range=(-1 + 1e-6, 1 - 1e-6)).fit(df[num_feats])\n",
    "\n",
    "            num_df = pd.DataFrame(data=minmax_skl.transform(df[num_feats]),\n",
    "                                  columns=num_feats)\n",
    "\n",
    "            # minmax_skl can be used on data with min max different\n",
    "            # than the data it used to fit on, so we need to clip it\n",
    "            num_df = np.clip(a=num_df, a_min=-1 + 1e-6, a_max=1 - 1e-6)\n",
    "\n",
    "        # Use Inverse of error function to shape like gaussian\n",
    "        for f_ in num_feats:\n",
    "            num_df[f_] = erfinv(num_df[f_].values)\n",
    "            the_mean = num_df[f_].mean()\n",
    "            num_df[f_] -= the_mean\n",
    "            # print(f_, the_mean)\n",
    "\n",
    "        return csr_matrix(num_df[num_feats].values), minmax_skl\n",
    "    else:\n",
    "        return csr_matrix(df[num_feats].values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataManager(object):\n",
    "    def __init__(self, mode, ratio):\n",
    "        self.mode = mode\n",
    "        self.idx = None\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def get_train_data(self):\n",
    "        if self.mode in [PROD, PROD_OOF]:\n",
    "            #train_idをidに変更\n",
    "            train = pd.read_table('../input/train.tsv', engine='c').rename(columns={\"train_id\": \"id\"})\n",
    "            train.ix[train.brand_name == 'PINK', 'brand_name'] = 'PINKBRAND'\n",
    "        elif self.mode in [STAGE2_OOF, STAGE2_PROD]:\n",
    "            train = pd.read_table('../input/train.tsv', engine='c').head(500000).rename(columns={\"train_id\": \"id\"})\n",
    "            train.ix[train.brand_name == 'PINK', 'brand_name'] = 'PINKBRAND'\n",
    "        elif self.mode == FAST_VALID:\n",
    "            np.random.seed(0)\n",
    "            data = pd.read_table('../input/train.tsv', engine='c')\n",
    "            if self.idx is None:\n",
    "                self.idx = np.arange(data.shape[0])\n",
    "                np.random.shuffle(self.idx)\n",
    "            data = data.iloc[self.idx]\n",
    "            train = data.head(int(data.shape[0] * (1 - self.ratio) / 10)).rename(columns={\"train_id\": \"id\"})\n",
    "            # test = data.tail(int(data.shape[0] *self. ratio / 10)).rename(columns={\"train_id\": \"id\"})\n",
    "            del data\n",
    "            gc.collect()\n",
    "        else:\n",
    "            # Use train for train and test\n",
    "            # Used to check the whole process fully works and scores are fine\n",
    "            # in particular makes sure train and test matrices in all steps are in sync\n",
    "            np.random.seed(0)\n",
    "            data = pd.read_table('../input/train.tsv', engine='c')\n",
    "            if self.idx is None:\n",
    "                self.idx = np.arange(data.shape[0])\n",
    "                np.random.shuffle(self.idx)\n",
    "            data = data.iloc[self.idx]\n",
    "            train = data.head(int(data.shape[0] * (1 - self.ratio))).rename(columns={\"train_id\": \"id\"})\n",
    "            del data\n",
    "            gc.collect()\n",
    "        return train\n",
    "\n",
    "    def get_test_data(self):\n",
    "        if self.mode in [PROD, PROD_OOF]:\n",
    "            test = pd.read_table('../input/test.tsv', engine='c').rename(columns={\"test_id\": \"id\"})\n",
    "            test.ix[test.brand_name == 'PINK', 'brand_name'] = 'PINKBRAND'\n",
    "        elif self.mode in [STAGE2_OOF, STAGE2_PROD]:\n",
    "            test = pd.read_table('../input/test.tsv', engine='c').rename(columns={\"test_id\": \"id\"})\n",
    "            test.ix[test.brand_name == 'PINK', 'brand_name'] = 'PINKBRAND'\n",
    "            test = simulate_test(test)\n",
    "        elif self.mode == FAST_VALID:\n",
    "            np.random.seed(0)\n",
    "            data = pd.read_table('../input/train.tsv', engine='c')\n",
    "            if self.idx is None:\n",
    "                self.idx = np.arange(data.shape[0])\n",
    "                np.random.shuffle(self.idx)\n",
    "            data = data.iloc[self.idx]\n",
    "            test = data.tail(int(data.shape[0] * self.ratio / 10)).rename(columns={\"train_id\": \"id\"})\n",
    "            del data\n",
    "            gc.collect()\n",
    "        else:\n",
    "            # Use train for train and test\n",
    "            # Used to check the whole process fully works and scores are fine\n",
    "            # in particular makes sure train and test matrices in all steps are in sync\n",
    "            np.random.seed(0)\n",
    "            data = pd.read_table('../input/train.tsv', engine='c')\n",
    "            if self.idx is None:\n",
    "                self.idx = np.arange(data.shape[0])\n",
    "                np.random.shuffle(self.idx)\n",
    "            data = data.iloc[self.idx]\n",
    "            test = data.tail(int(data.shape[0] * self.ratio)).rename(columns={\"train_id\": \"id\"})\n",
    "            del data\n",
    "            gc.collect()\n",
    "        return test\n",
    "\n",
    "\n",
    "def add_name_features_for_train(df=None):\n",
    "    indexers = []\n",
    "    for i in range(6):\n",
    "        # print(f_ + \"_\" + str(i))\n",
    "        df[\"fact_name_\" + str(i)] = df[\"name\"].fillna(\"\").str.split().str[i].fillna(\"no_name\")\n",
    "        df[\"fact_name_\" + str(i)], indexer = pd.factorize(df[\"fact_name_\" + str(i)])\n",
    "        indexers.append(indexer)\n",
    "        gc.collect()\n",
    "\n",
    "    return indexers\n",
    "\n",
    "\n",
    "def add_name_features_for_test(df=None, indexers=None):\n",
    "    for i in range(6):\n",
    "        # print(f_ + \"_\" + str(i))\n",
    "        df[\"fact_name_\" + str(i)] = df[\"name\"].fillna(\"\").str.split().str[i].fillna(\"no_name\")\n",
    "        df[\"fact_name_\" + str(i)] = indexers[i].get_indexer(df[\"fact_name_\" + str(i)])\n",
    "\n",
    "\n",
    "def get_sgd_oof_predictions(csr_ridge_trn, folds, models_list, y):\n",
    "    len_pred = csr_ridge_trn.shape[0]\n",
    "    liblinear_preds = np.zeros(len_pred)\n",
    "    ridge_preds = np.zeros(len_pred)\n",
    "    for fold_n, (trn_idx, val_idx) in enumerate(folds.split(csr_ridge_trn)):\n",
    "        liblinear_preds[val_idx] = models_list[fold_n * 2].predict(csr_ridge_trn[val_idx])\n",
    "        ridge_preds[val_idx] = models_list[fold_n * 2 + 1].predict(csr_ridge_trn[val_idx])\n",
    "        if y is not None:\n",
    "            score_liblinear = mean_squared_error(y[val_idx], liblinear_preds[val_idx]) ** .5\n",
    "            print(\"SGD L2 Fold %2d : %.6f\" % (fold_n + 1, score_liblinear))\n",
    "            score_ridge = mean_squared_error(y[val_idx], ridge_preds[val_idx]) ** .5\n",
    "            print(\"SGD L1 Fold %2d : %.6f\" % (fold_n + 1, score_ridge))\n",
    "    return liblinear_preds, ridge_preds\n",
    "\n",
    "\n",
    "def get_sgd_test_predictions(csr_ridge_sub, folds, models_list, y=None):\n",
    "    len_pred = csr_ridge_sub.shape[0]\n",
    "    liblinear_preds = np.zeros(len_pred)\n",
    "    ridge_preds = np.zeros(len_pred)\n",
    "    for fold_n in range(folds.n_splits):\n",
    "        liblinear_preds += models_list[fold_n * 2].predict(csr_ridge_sub) / folds.n_splits\n",
    "        ridge_preds += models_list[fold_n * 2 + 1].predict(csr_ridge_sub) / folds.n_splits\n",
    "    if y is not None:\n",
    "        score_liblinear = mean_squared_error(y, liblinear_preds) ** .5\n",
    "        print(\"liblinear Test  : %.6f\" % score_liblinear)\n",
    "        score_ridge = mean_squared_error(y, ridge_preds) ** .5\n",
    "        print(\"SGD Test L1 : %.6f\" % score_ridge)\n",
    "    return liblinear_preds, ridge_preds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    53\n",
      "1    47\n",
      "Name: shipping, dtype: int64\n",
      "3    42\n",
      "1    41\n",
      "2    16\n",
      "4     1\n",
      "Name: item_condition_id, dtype: int64\n",
      "[7.344279050827026] Finished to load train data  train shape:  (100, 8)\n",
      "[7.7311201095581055] Finished to handle missing\n",
      "[9.837897300720215] Finished to add text lengths in train and test\n",
      "d_no d_description d_yet\n",
      "d_great d_quality d_<!> d_<!> d_<!> d_fast d_free d_shipping d_<!> d_<!> d_you d_can d_find d_more d_beautiful d_items d_in d_my d_listings d_for d_a d_great d_affordable d_price d_<!> d_<!> d_everything d_is d_sell d_is d_new d_and d_for d_free d_shipping d_<!> d_<!> d_i d_also d_make d_bundles d_<!> d_<!>\n",
      "d_this d_keyboard d_is d_in d_great d_condition d_and d_works d_like d_it d_came d_out d_of d_the d_box d_<.> d_all d_of d_the d_ports d_are d_tested d_and d_work d_perfectly d_<.> d_the d_lights d_are d_customizable d_via d_the d_razer d_synapse d_app d_on d_your d_pc d_<.>\n",
      "d_adorable d_top d_with d_a d_hint d_of d_lace d_and d_a d_key d_hole d_in d_the d_back d_<!> d_the d_pale d_pink d_is d_a d_1x d_<,> d_and d_i d_also d_have d_a d_3x d_available d_in d_white d_<!>\n",
      "d_new d_with d_tags d_<.> d_leather d_horses d_<.> d_retail d_for d_<[> d_rm d_<]> d_each d_<.> d_stand d_about d_a d_foot d_high d_<.> d_they d_are d_being d_sold d_as d_a d_pair d_<.> d_any d_questions d_please d_ask d_<.> d_free d_shipping d_<.> d_just d_got d_out d_of d_storage\n",
      "d_complete d_with d_certificate d_of d_authenticity\n",
      "d_banana d_republic d_bottoms d_<,> d_candies d_skirt d_with d_matching d_blazer d_<,> d_amy d_byers d_suit d_<,> d_loft d_bottoms d_and d_cami d_top d_<.>\n",
      "d_size d_small d_but d_straps d_slightly d_shortened d_to d_fit d_xs d_<,> d_besides d_that d_<,> d_perfect d_condition\n",
      "d_you d_get d_three d_pairs d_of d_sophie d_cheer d_shorts d_size d_small d_and d_medium d_girls d_and d_two d_sports d_bra d_</> d_boy d_shorts d_spandex d_matching d_sets d_in d_small d_and d_medium d_girls d_<.> d_all d_items d_total d_retail d_for d_<[> d_rm d_<]> d_in d_store d_and d_you d_can d_take d_him d_today d_for d_less d_than d_the d_price d_of d_one d_item d_at d_the d_store d_<!> d_<)>\n",
      "d_girls d_size d_small d_plus d_green d_<.> d_three d_shorts d_total d_<.>\n",
      "d_i d_realized d_his d_pants d_are d_on d_backwards d_after d_the d_picture d_<.> d_they d_were d_very d_dirty d_so d_i d_hand d_washed d_them d_<.> d_he d_has d_a d_stuffed d_body d_and d_painted d_porcelain d_head d_<,> d_hands d_and d_feet d_<.> d_back d_before d_clowns d_were d_too d_scary d_<.> d_9 d_<\"> d_tall d_<.> d_no d_chips d_or d_cracks d_but d_minor d_paint d_loss d_in d_a d_few d_places d_<.> d_clown d_circus d_doll d_collectible\n",
      "d_0 d_<.> d_25 d_oz d_full d_size d_is d_1 d_oz d_for d_<[> d_rm d_<]> d_in d_sephora\n",
      "d_<(> d_5 d_<)> d_new d_vs d_pink d_body d_mists d_<(> d_2 d_<.> d_5 d_oz d_each d_<)> d_fresh d_& d_clean d_sun d_kiss d_cool d_and d_bright d_total d_flirt d_sweet d_and d_flirty\n",
      "d_xl d_<,> d_great d_condition\n",
      "d_no d_description d_yet\n",
      "d_authentic d_<.> d_suede d_fringe d_boots d_<.> d_great d_condition d_<!> d_size d_7 d_<.> d_if d_you d_are d_between d_the d_sizes d_5 d_<.> d_5 d_<-> d_7 d_and d_love d_wearing d_thick d_socks d_during d_the d_winter d_they'd d_be d_perfect d_for d_you d_as d_well d_<(> d_i d_did d_last d_winter d_<)> d_<:> d_<)>\n",
      "d_brand d_new d_<.> d_deluxe d_travel d_size d_products d_<.> d_contains d_<:> d_amazonian d_clay d_12 d_hour d_blush d_in d_paaarty d_<-> d_<.> d_05 d_oz d_</> d_1 d_<.> d_5g d_tarteist d_lip d_paint d_in d_birthday d_suit d_<-> d_<.> d_034 d_oz d_</> d_1ml\n",
      "d_2 d_glitter d_eyeshadows d_<;> d_one d_in d_brass d_and d_one d_in d_bleached d_<.>\n",
      "d_brand d_new d_in d_box d_size d_<:> d_medium d_color d_<:> d_coral d_retails d_for d_<[> d_rm d_<]> d_the d_baby d_k'tan d_active d_is d_made d_of d_a d_breathable d_hi d_<-> d_tech d_performance d_fabric d_that d_wicks d_away d_moisture d_and d_sweat d_<,> d_blocks d_over d_90 d_percent d_of d_the d_sun's d_harmful d_uva d_and d_uvb d_rays d_<,> d_and d_provides d_a d_unique d_temperature d_control d_<.> d_<-> d_ergonomic d_positioning d_for d_healthy d_infant d_development d_<.> d_<-> d_evenly d_distributes d_weight d_across d_back d_and d_shoulders d_<.> d_<-> d_double d_<-> d_loop d_design d_slips d_on d_like d_a d_t d_<-> d_shirt d_<.>\n",
      "d_they d_are d_100 d_percent d_authentic d_<.> d_they d_are d_beaters d_but d_they d_still d_have d_a d_lot d_of d_life d_in d_them d_<.> d_no d_original d_box d_<.>\n",
      "d_brand d_new d_otterbox d_defender d_iphone d_6 d_plus d_</> d_6s d_plus\n",
      "d_this d_authentic d_pallete d_by d_too d_faced d_is d_brand d_new d_in d_mint d_condition d_still d_in d_original d_box d_<.> d_it's d_part d_of d_the d_<.> d_christmas d_2016 d_collection d_<.> d_it d_has d_12 d_pretty d_eye d_shadow d_colors d_and d_a d_small d_sized d_<\"> d_better d_than d_sex d_<\"> d_mascara d_<.> d_never d_even d_swatched d_<.> d_impeccable d_shape d_<.> d_price d_includes d_2 d_day d_priority d_shipping d_with d_insurance d_<.>\n",
      "d_worn d_one d_time d_<.> d_excellent d_condition\n",
      "d_fancy d_<,> d_dressy d_or d_casual d_<!> d_dress d_it d_up d_or d_down d_100 d_percent d_polyester d_<;> d_washed d_once d_<,> d_never d_dried d_<.> d_size d_<:> d_small d_brand d_<:> d_lush d_purchased d_from d_francesca's d_tags d_<:> d_free d_people d_<,> d_anthropology d_<,> d_dry d_goods d_<,> d_francesca's\n",
      "d_beautiful d_excellent d_condition d_zips d_and d_ties d_in d_the d_back d_cream d_liner d_top d_to d_bottom\n",
      "d_size d_1 d_<.> d_worn d_once d_<.> d_excellent d_condition\n",
      "d_nwt d_victoria's d_secret d_ultimate d_sport d_bra d_<-> d_maximum d_support d_size d_34ddd\n",
      "d_29w d_<.> d_x d_33l d_<.> d_social d_stretch d_hollister d_jeans d_<*> d_<*> d_<*> d_please d_see d_picture d_4 d_<,> d_the d_leathers d_tag d_near d_the d_belt d_loops d_is d_tore d_also d_<,> d_ae d_favorite d_boyfriend d_size d_8 d_stretch\n",
      "d_reasonable d_offers d_welcomed d_<.> d_but d_if d_you d_ask d_<\"> d_lowest d_<\"> d_or d_lowball d_i'll d_block d_you d_<.> d_<.> d_<.> d_<-> d_this d_phone d_was d_opened d_under d_t d_<-> d_mobile d_but d_has d_now d_been d_unlocked d_after d_switching d_to d_new d_phone d_<.> d_<-> d_all d_of d_the d_buttons d_are d_functioning d_like d_new d_still d_<.> d_<-> d_it's d_in d_perfect d_used d_condition d_<-> d_check d_my d_reviews d_for d_honest d_feedback d_<.> d_<-> d_32 d_gb d_<;> d_sorry d_no d_charger d_included\n",
      "d_bnib d_3 d_for d_<[> d_rm d_<]> d_better d_than d_sex d_waterproof d_mascara d_travel d_size d_4 d_<.> d_8g d_</> d_0 d_<.> d_17 d_oz d_see d_my d_other d_posts d_for d_<[> d_rm d_<]> d_each d_or d_2 d_for d_<[> d_rm d_<]>\n",
      "d_brand d_new d_never d_used d_all d_colors d_are d_available d_each d_only d_<[> d_rm d_<]>\n",
      "d_no d_description d_yet\n",
      "d_pink d_bra d_with d_logo d_band d_<!> d_36d d_push d_up\n",
      "d_lanascloset d_<~> d_<~> d_<~> d_description d_<:> d_never d_worn d_<!> d_i d_delete d_</> d_update d_my d_listings d_and d_relist d_them d_so d_like d_my d_<\"> d_sold d_<\"> d_listings d_to d_have d_easier d_access d_to d_my d_shop d_later d_on d_<~> d_<~> d_<~> d_i d_normally d_ship d_the d_following d_day d_<,> d_but d_it d_happens d_that d_i d_ship d_a d_few d_days d_after d_purchase d_<~> d_<~> d_<~> d_forever d_21 d_brandy d_melville d_baseball d_tee\n",
      "d_new d_unused d_and d_authentic d_<.> d_caudalie d_beauty d_elixir d_mist d_<.> d_1 d_oz\n",
      "d_2 d_beanie d_babies d_pugsley d_wrinkles d_puppy d_with d_pumpkin d_big d_dog d_retro d_pinup d_doll d_frilly d_skirted d_adorable d_barbie d_pinksuper d_cute d_<!> d_fan d_martini d_cherry d_pinup d_anchor d_marabou d_<#> d_kitschoure d_<#> d_beasweetlollipopinaworldofsoursuckers d_from d_love d_in d_sunny d_san d_diego d_cali d_<-> d_fornia d_us d_of d_a d_<!> d_absolutely d_adorable d_soft d_genuine d_real d_bunny d_fox d_foxy d_loxy d_fur d_stunning d_and d_gorgeous d_<!> d_bambi d_long d_eyelashes d_<!> d_sweet d_cheeks d_blush d_pink d_juicy d_dollface d_<!> d_candy d_colors d_<!> d_girlfriend d_<.> d_dance d_club d_baby d_<!> d_panty d_present d_for d_the d_girl d_who d_has d_everything d_<!> d_cupcake d_couture d_resin d_kawaii d_lolita d_dress d_up d_gift d_for d_lime d_crime d_pegasus d_unicorn d_flamingo d_swan d_princess d_kitty d_pinup d_rockabilly d_girls d_jewelry d_and d_wild d_fox d_lolita d_kawaii d_gypsy d_wedding d_festival d_edf d_electronic d_dance d_rave d_raver d_coachella d_party\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_one d_size d_fits d_sizes d_2 d_<-> d_12 d_92 d_percent d_polyester d_8 d_percent d_spandex d_super d_soft d_<!> d_<!> d_capri d_leggings d_high d_waist d_1in d_elastic d_waist d_band\n",
      "d_pre d_<-> d_owned d_<.> d_some d_light d_scratches d_on d_hardware d_consistent d_with d_gentle d_use d_<.> d_real d_togo d_leather d_<.> d_comes d_with d_twilly d_<.>\n",
      "d_has d_some d_wear d_to d_the d_back d_<,> d_and d_few d_white d_spots d_which d_i d_believe d_can d_be d_washed d_out\n",
      "d_500 d_gb d_comes d_with d_power d_cord d_one d_controller d_with d_rechargeable d_pack d_and d_cord d_comes d_with d_downloaded d_games d_the d_top d_is d_pretty d_scratched d_up d_but d_it d_doesn't d_affect d_the d_gameplay\n",
      "d_victoria d_secret d_34 d_c d_corest d_top d_will d_bundle d_to d_save d_on d_shipping d_if d_you d_have d_any d_questions d_please d_feel d_free d_to d_ask\n",
      "d_kylie d_happy d_birthday d_matte d_mini d_lipstick d_kylie d_jenner d_lip d_kit d_birthday d_matte d_lipstick d_mini d_kit d_6 d_pcs d_all d_6 d_included d_for d_the d_price d_<.> d_brand d_new d_never d_opened d_6 d_mini d_lipstick d_leo d_kristen d_dolce d_expos d_price d_is d_firm d_<!> d_will d_not d_be d_disappointed d_shipping d_generally d_takes d_1 d_<-> d_3 d_days d_via d_usps d_i d_ship d_within d_24 d_hours d_<.> d_48 d_hours d_on d_week\n",
      "d_used d_buh d_looks d_brand d_new d_and d_is d_washed d_and d_ready d_to d_be d_shipped d_supper d_cute d_tropical d_print d_supper d_comfy d_and d_soft d_size d_<:> d_34 d_aa d_great d_quality d_made d_by d_victoria's d_secret d_pink d_100 d_percent d_authentic d_called d_<\"> d_where d_every d_where d_lightly d_lined d_bra d_<\"> d_free d_ship d_ships d_same d_day d_price d_includes d_shipping d_and d_seller d_fee\n",
      "d_totally d_36 d_masks d_<,> d_will d_be d_expired d_on d_feb d_<.> d_<.>\n",
      "d_brand d_new d_<!> d_never d_used d_smoking d_bowl d_<.> d_just d_bought d_2 d_days d_ago d_<.> d_can't d_return d_these d_things d_after d_you d_buy d_<.> d_pretty d_christmas d_colors d_<:> d_<)> d_too d_big d_for d_my d_piece d_<.>\n",
      "d_black d_outside d_medium d_gray d_inside d_<.> d_authentic d_<,> d_super d_warm d_and d_in d_like d_new d_condition d_<.> d_size d_small d_and d_runs d_true d_to d_size d_<.>\n",
      "d_this d_slime d_is d_approximately d_1 d_<.> d_5 d_ounces d_<,> d_very d_soft d_<,> d_it d_is d_kind d_of d_sticky d_<,> d_this d_item d_also d_comes d_in d_an d_air d_tight d_container d_for d_easy d_storage d_<.> d_this d_slime d_was d_scemted d_with d_sparkling d_blackberry d_woods d_by d_bath d_and d_body d_works d_<.>\n",
      "d_brandy d_melville d_off d_shoulder d_crop d_top d_<.> d_one d_size d_fits d_all d_<.> d_euc d_<.> d_3 d_</> d_4 d_sleeves d_<.> d_100 d_percent d_rayon d_<.> d_actual d_color d_<:> d_navy d_blue d_and d_cream d_in d_stripes d_<.>\n",
      "d_perfect d_condition d_<!> d_super d_comfy d_<,> d_let d_me d_know d_if d_you d_have d_any d_questions d_<!> d_<*> d_let d_me d_know d_if d_you d_want d_any d_more d_pictures d_<*> d_please d_ask d_for d_a d_measurement d_of d_you d_are d_not d_sure d_about d_your d_size d_<*> d_bundle d_discount d_<(> d_<:> d_<*> d_<*> d_usually d_ship d_in d_the d_same d_day d_if d_purchased d_on d_monday d_<-> d_saturday d_post d_office d_hours d_<*> d_<*> d_any d_issues d_after d_purchase d_<.> d_<.> d_<.> d_please d_contact d_me d_as d_soon d_as d_you d_have d_found d_the d_issue d_<.> d_your d_satisfaction d_is d_my d_concern d_<.> d_<*> d_<*>\n",
      "d_younique d_3d d_fiber d_lash d_mascara d_will d_quickly d_become d_your d_favorite d_mascara d_<.> d_<.> d_<.> d_it d_will d_increase d_your d_lashes d_by d_300 d_percent d_<.> d_<.> d_<.> d_brand d_new d_in d_package d_never d_been d_opened d_or d_used d_<.> d_<.> d_free d_shipping d_ships d_within d_24hrs\n",
      "d_sz d_large d_color d_black d_like d_new d_100 d_percent d_authentic\n",
      "d_these d_are d_on d_hold d_do d_not d_buy d_<:> d_<)>\n",
      "d_led d_shoes d_for d_kids d_<,> d_seven d_colors d_in d_one d_shoes d_<,> d_red d_<,> d_blue d_<,> d_green d_<,> d_purple d_<,> d_turquaz d_<,> d_yellow d_<,> d_white d_<,> d_and d_6 d_different d_flashlight d_options d_<.> d_sizes d_are d_available d_<,> d_unisex d_sizes d_boys d_or d_girl d_<.> d_available d_sizes d_are d_from d_10 d_to d_3\n",
      "d_both d_are d_a d_chinese d_size d_medium d_which d_fits d_a d_us d_size d_xs d_<.> d_both d_are d_brand d_new d_<,> d_never d_worn d_<,> d_did d_not d_come d_with d_a d_sanitary d_liner d_<.> d_black d_ones d_are d_more d_cheeky d_then d_the d_burgundy d_ones d_<.> d_will d_sell d_separately\n",
      "d_bundle d_for d_<\"> d_carl's d_collectibles d_<\"> d_<.> d_year d_<:> d_2000 d_mattel d_<,> d_inc d_<.> d_bundle d_<:> d_elvis d_<\"> d_favorite d_cars d_<\"> d_collection d_and d_vintage d_hot d_rids d_by d_hot d_wheels d_<.>\n",
      "d_aura d_fluorite d_<:> d_<[> d_<?> d_<]> d_the d_<\"> d_protection d_stone d_<\"> d_<[> d_<?> d_<]> d_grounding d_<[> d_<?> d_<]> d_spiritualizing d_<[> d_<?> d_<]> d_intuition d_<[> d_<?> d_<]> d_the d_stone d_to d_look d_for d_when d_wishing d_to d_connect d_with d_the d_spirit d_details d_<:> d_<[> d_<?> d_<]> d_4 d_3 d_</> d_4 d_<\"> d_<[> d_<?> d_<]> d_pure d_aura d_fluorite d_<[> d_<?> d_<]> d_cleansed d_and d_charged d_have d_a d_blessed d_day d_<~> d_tags d_<:> d_witchcraft d_<,> d_witch d_<,> d_altar d_<,> d_crystal d_<,> d_wiccan d_<,> d_wicca d_<,> d_pagan d_<,> d_witchcraft d_artifact d_<,> d_witchcraftartifact d_<,> d_chakra\n",
      "d_victoria's d_secret d_pink d_white d_</> d_cream d_colored d_lace d_strapless d_bandeau d_<.> d_size d_small d_<.> d_it d_doesn't d_have d_a d_specific d_cup d_size d_<.> d_message d_me d_with d_questions d_<!>\n",
      "d_worn d_once d_<;> d_will d_be d_washed d_before d_sent\n",
      "d_great d_condition d_sea d_wees d_size d_0 d_brown\n",
      "d_new d_2017 d_rae d_dunn d_holiday d_<\"> d_hugs d_<\"> d_mug d_<.> d_another d_great d_holiday d_mug d_<.> d_keep d_or d_give d_to d_someone d_special d_<.> d_we d_pack d_with d_care d_and d_ship d_promptly d_<.> d_please d_inspect d_your d_order d_when d_delivered d_<.> d_help d_complete d_your d_order d_by d_rating d_transaction d_<.>\n",
      "d_all d_are d_made d_out d_of d_wood d_<.> d_necklace d_<,> d_earrings d_broach d_<.> d_all d_sold d_together d_<.>\n",
      "d_sheer d_black d_flowy d_top d_with d_cute d_flower d_design d_<.> d_ties d_at d_the d_back d_or d_front d_<.> d_size d_medium d_<.> d_great d_used d_condition d_<.>\n",
      "d_new d_pure d_honey d_5 d_oz\n",
      "d_new d_without d_original d_packaging d_<.> d_shade d_is d_medium d_it d_cosmetics d_by d_jamie d_kern d_<.> d_full d_size d_container d_<.> d_10 d_ml d_shade d_is d_medium d_<.> d_cc d_<+> d_eye d_full d_coverage d_cream d_color d_corrector d_correcting d_cream d_<.> d_if d_you d_are d_looking d_for d_certain d_brands d_or d_products d_<.> d_<.> d_<.> d_and d_don't d_have d_time d_for d_meandering d_through d_all d_my d_products d_<.> d_<.> d_<.> d_search d_for d_my d_products d_using d_the d_search d_function d_<.> d_in d_the d_search d_space d_above d_type d_in d_plussizemakeup d_this d_will d_bring d_up d_all d_of d_my d_listings d_so d_you d_can d_search d_my d_listings d_<.> d_<.> d_<.> d_you d_can d_even d_refine d_your d_search d_by d_category d_and d_brands d_<!> d_<!> d_<#> d_plussizemakeup\n",
      "d_great d_condition d_<!> d_no d_stains d_or d_tears d_super d_easy d_to d_use d_<!> d_by d_infantino d_brand d_<!> d_purchased d_at d_target d_free d_shipping d_<:> d_<)>\n",
      "d_5 d_mascaras d_loreal d_covergirl d_rimmel d_maybelline\n",
      "d_eyebrows d_essential d_kit d_everything d_you d_need d_to d_create d_perfect d_eyebrows d_packed d_in d_one d_convenient d_package d_<.> d_with d_easy d_to d_follow d_directions d_<,> d_it d_enables d_you d_to d_have d_the d_natural d_looking d_brows d_without d_harsh d_line d_that d_enhance d_your d_features d_<.> d_brows d_essential d_kit d_consists d_of d_<:> d_1 d_eyebrow d_powder d_3 d_eyebrow d_stencils d_in d_thin d_<,> d_natural d_and d_thick d_<,> d_1 d_eyebrow d_brush d_applicator d_used d_to d_apply d_the d_eyebrow d_powder d_precisely d_to d_the d_desired d_effect d_<.>\n",
      "d_new\n",
      "d_new d_hv d_<-> d_900 d_sports d_sweatproof d_wireless d_bluetooth d_4 d_<.> d_0 d_stereo d_headsets d_earphone\n",
      "d_extra d_large d_shirt d_with d_purple d_beaded d_flower d_design d_<.> d_size d_extra d_large d_<.>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_hot d_pink d_</> d_reddish d_sheet d_set d_from d_victoria's d_secret d_pink d_store d_<.> d_in d_good d_condition d_<,> d_some d_fading d_from d_washing d_but d_fully d_intact d_<.> d_includes d_flat d_sheet d_and d_two d_pillow d_cases d_<,> d_no d_fitted d_sheet d_<!> d_design d_is d_white d_hearts d_with d_light d_pink d_cheetah d_print d_trim d_<.> d_fits d_queen d_sized d_bed d_<!>\n",
      "d_this d_is d_a d_very d_beautiful d_diamond d_engagement d_ring d_size d_7 d_with d_a d_thicker d_band d_than d_most d_solitaires d_<.> d_reasonable d_offers d_accepted d_<!>\n",
      "d_this d_boxed d_set d_of d_12 d_dvd's d_includes d_the d_fitness d_guide d_<,> d_meal d_plan d_<,> d_and d_rotational d_calendar d_<.> d_the d_dvd's d_include d_<:> d_fire d_up d_<,> d_ignite d_<,> d_launch d_<,> d_rise d_<,> d_amplify d_<,> d_escalate d_<,> d_conquer d_<,> d_triumph d_<,> d_zenith d_<,> d_and d_apex d_<.> d_includes d_bonus d_dvd d_opus d_and d_the d_cast d_& d_moves d_disc d_<.> d_original d_dvds d_sealed d_in d_box d_<.> d_ships d_out d_within d_24 d_hours d_ignore d_<:> d_insanity d_p90x d_work d_out d_workout d_abs d_muscles d_bodyshred\n",
      "d_brand d_new d_with d_tag d_and d_bag d_<.> d_french d_bulldog d_leggings d_<.> d_hard d_to d_find d_this d_color d_background d_<.> d_super d_soft d_<!> d_<!> d_made d_in d_vietnam d_<.> d_as d_always d_<,> d_free d_shipping d_<.> d_check d_out d_my d_other d_listings d_for d_bundling d_<.>\n",
      "d_used d_but d_still d_a d_great d_play d_item d_<.>\n",
      "d_distressed d_levi d_high d_waist d_jeans d_<.> d_size d_xs d_<,> d_12 d_slim d_in d_kids d_size\n",
      "d_highwaist d_distressed d_denim d_shorts d_size d_3 d_the d_shorts d_i d_wear d_in d_most d_of d_my d_images\n",
      "d_i d_have d_2 d_available\n",
      "d_men's d_xxl d_in d_good d_condition d_minor d_mark d_on d_sleeve\n",
      "d_size d_6y d_<.> d_high d_tops d_<.> d_super d_light d_<.> d_in d_good d_condition d_<.> d_worn d_for d_1 d_baske d_tb d_all d_season d_<.> d_don't d_fit d_my d_sin d_anymore d_<.>\n",
      "d_no d_description d_yet\n",
      "d_brand d_new\n",
      "d_good d_used d_condition d_<.> d_ask d_about d_bundling d_to d_save d_<.>\n",
      "d_signed d_italy d_and d_925 d_necklace d_vintage d_<,> d_lobster d_claw d_closure d_16 d_inches d_long\n",
      "d_distressed d_with d_holes d_<.> d_great d_shape d_well d_taken d_care d_of d_<!> d_fits d_a d_size d_11 d_<,> d_perfect d_for d_spring d_summer d_and d_fall d_<!>\n",
      "d_<-> d_size d_xs d_<(> d_fits d_long d_<)> d_<,> d_or d_s d_<(> d_fits d_medium d_length d_<)> d_<-> d_rarely d_worn d_<,> d_great d_condition d_<-> d_no d_flaws d_<-> d_selling d_b d_</> d_c d_i d_just d_don't d_wear d_it d_<-> d_35 d_percent d_cotton d_<,> d_37 d_percent d_viscose d_<,> d_18 d_percent d_polyamide d_<,> d_10 d_percent d_angora d_<-> d_retail d_<[> d_rm d_<]>\n",
      "d_size d_m d_<(> d_8 d_<)> d_<,> d_cuffs d_show d_wear d_and d_letters d_are d_not d_peeling d_off d_it d_came d_like d_that d_<.>\n",
      "d_under d_armour d_half d_zip d_jacket d_in d_awesome d_like d_new d_condition d_<!> d_size d_med d_<.>\n",
      "d_bnip d_vhtf d_<!> d_hottest d_toy d_of d_the d_season d_<!>\n",
      "d_please d_check d_size d_before d_ordering d_if d_you d_have d_no d_ratings d_contact d_me d_after d_you d_purchase d_or d_i d_will d_cancel d_<[> d_<?> d_<]> d_<[> d_<?> d_<]> d_provides d_great d_protection d_from d_scratches d_and d_drops d_<!> d_<[> d_<?> d_<]> d_<[> d_<?> d_<]> d_price d_is d_extremely d_firm d_<.> d_all d_offers d_will d_be d_ignored d_<[> d_<?> d_<]> d_<[> d_<?> d_<]> d_brand d_new d_in d_factory d_sealed d_packaging d_<!> d_<[> d_<?> d_<]> d_<[> d_<?> d_<]> d_will d_be d_shipped d_out d_asap d_<[> d_<?> d_<]> d_<[> d_<?> d_<]> d_check d_out d_my d_page d_for d_different d_colors d_<,> d_and d_a d_large d_variety d_of d_cases d_<!> d_<[> d_<?> d_<]> d_<[> d_<?> d_<]> d_allow d_1 d_<-> d_2 d_days d_for d_tracking d_to d_update d_<!> d_<!>\n",
      "d_overall d_good d_condition d_<.> d_a d_few d_signs d_of d_wear\n",
      "d_babygap d_canvas d_army d_green d_gently d_loved d_<,> d_lots d_of d_life d_left d_no d_holes d_throw d_in d_the d_wash d_& d_hang d_dry d_<,> d_or d_wipe d_them d_clean d_<.> d_girl d_or d_boy d_<,> d_my d_son d_wore d_these d_with d_khakis d_to d_church\n",
      "d_<\"> d_fine d_or d_fashion d_<:> d_fashion d_item d_type d_<:> d_necklace d_chain d_length d_<:> d_60 d_<+> d_5 d_cm d_<\"> d_shipping d_from d_china d_<,> d_competitive d_price d_<,> d_shipping d_takes d_about d_15 d_days d_<~>\n",
      "d_bought d_off d_groupon d_and d_they d_charged d_me d_for d_2 d_<,> d_sent d_2 d_<,> d_and d_they d_won't d_return d_one d_even d_though d_it d_was d_a d_system d_error d_<!> d_it's d_still d_new d_and d_completely d_sealed d_<.> d_the d_second d_picture d_shows d_everything d_that's d_in d_it d_and d_the d_regular d_cost d_<.> d_it d_also d_comes d_with d_a d_how d_<-> d_to d_dvd d_<.> d_i d_ordered d_the d_<\"> d_medium d_shade d_<\"> d_as d_well d_<.> d_comes d_with d_4 d_different d_shades d_<\"> d_to d_find d_your d_perfect d_match d_<\"> d_<,> d_a d_face d_primer d_<,> d_a d_brightening d_color d_<,> d_and d_blush d_<.> d_you d_can d_buy d_different d_shades d_off d_their d_website d_too d_<.>\n",
      "d_warmers d_pictured d_& d_6 d_sample d_packages d_of d_highly d_scented d_wax d_melts d_nib d_full d_size d_flameless d_electric d_scented d_wax d_melt d_</> d_tart d_</> d_essential d_oil d_warmer d_lamp d_candle d_burner d_with d_25 d_watt d_bulb d_included d_<!> d_guaranteed d_superior d_quality d_<,> d_safe d_durable d_craftsmanship d_<,> d_affordable d_home d_fragrance d_solutions d_<~> d_3 d_<.> d_5 d_ft d_cord d_has d_on d_</> d_off d_switch d_<.> d_<~> d_amazing d_scent d_throw d_quickly d_releases d_aroma d_<~> d_doubles d_as d_night d_light d_<.> d_<~> d_perfect d_addition d_to d_every d_room d_in d_your d_home d_<,> d_dorm d_<,> d_business d_<,> d_waiting d_area d_<,> d_farm d_<,> d_house d_<,> d_ranch d_<,> d_cabin d_<,> d_<~> d_huge d_selection d_for d_personal d_and d_individualized d_themes d_</> d_decor d_</> d_decorating d_styles d_<.> d_<~> d_compatible d_with d_any d_brand d_scented d_wax d_melts d_tarts d_ie d_<.> d_scentsy d_bbw d_bath d_and d_body d_works d_yankee d_pink d_zebra d_bed d_bath d_beyond d_vs d_<~> d_comes d_with d_3 d_sample d_pkgs d_high d_quality d_scented d_wax d_melts d_<(> d_scents d_listed d_on d_profile d_page d_<)> d_additional d_melts d_can d_be d_added d_to d_order d_<@> d_<[> d_rm d_<]> d_per d_1 d_<.> d_7 d_to d_2 d_oz d_regular d_package d_please d_ask d_before d_ordering d_<:> d_<)> d_<~> d_please d_visit d_us d_again d_by d_following d_our d_page d_<:> d_<)>\n",
      "d_bling d_mickey d_ear d_case d_<!> d_<!> d_just d_used d_one d_for d_a d_disney d_trip d_<!> d_received d_many d_compliments d_<!> d_<!> d_bling d_ears d_<!> d_super d_cute d_<!> d_3 d_stones d_missing d_to d_be d_exact d_but d_barely d_noticeable d_<:> d_<)> d_comes d_with d_strap d_for d_neck d_<!>\n",
      "d_watercolor d_inspire d_crop d_<.> d_no d_pilling d_or d_stickiness d_whatsoever d_<.> d_some d_of d_the d_pink d_has d_bled d_into d_the d_white d_part d_of d_the d_wais d_tb d_and d_<,> d_not d_noticeable d_from d_a d_distance d_<.> d_22 d_<\"> d_inseam d_<.> d_rip d_tag d_still d_intact d_<,> d_size d_6 d_<.>\n",
      "d_probably d_best d_for d_up d_to d_a d_2 d_year d_old d_<.>\n",
      "d_rose d_gold d_bezel d_and d_crown d_engraved d_logo d_red d_and d_green d_signature d_band d_unisex d_no d_box\n",
      "d_brand d_new d_<!> d_no d_cracks d_or d_chips d_<!> d_i d_package d_everything d_with d_extra d_care d_<!> d_not d_responsible d_for d_item d_once d_shipped\n"
     ]
    }
   ],
   "source": [
    "gc.enable()\n",
    "\n",
    "# Set start time\n",
    "start_time = time.time()\n",
    "\n",
    "main_apply_series_man = ApplySeriesManager(nb_workers=4)\n",
    "main_apply_man = ApplyManager(nb_workers=4)\n",
    "main_hash_man = HashingManager(nb_workers=4)\n",
    "\n",
    "# Read training data and test dataset\n",
    "\n",
    "mode =PROD\n",
    "ratio  = 0.2 \n",
    "\n",
    "data_man = DataManager(mode=mode, ratio=ratio)\n",
    "train = data_man.get_train_data()\n",
    "\n",
    "train = train[:100]\n",
    "\n",
    "print(train[\"shipping\"].value_counts())\n",
    "print(train[\"item_condition_id\"].value_counts())\n",
    "# Remove 0 price in train dataset\n",
    "#値段が0のものを取り除く\n",
    "train = train.loc[train.price > 0]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Get target out of train  dataset\n",
    "#logを適用する\n",
    "y = np.log1p(train[\"price\"].values)\n",
    "\n",
    "print('[{}] Finished to load train data'.format(time.time() - start_time), ' train shape: ', train.shape)\n",
    "\n",
    "\n",
    "# Create price statistics by brand\n",
    "# brand price mappers should contain mapping for mean, std,\n",
    "#brand_nameについてとカテゴリについて統計値を計算しデータフレームに入れ込む\n",
    "train, brand_price_mappers = add_price_statistics_on_train(trn=train, target=y, feature=\"brand_name\")\n",
    "train, cat_price_mappers = add_price_statistics_on_train(trn=train, target=y, feature=\"category_name\")\n",
    "\n",
    "\n",
    "# Replace NaN\n",
    "#null値を埋める\n",
    "handle_missing_inplace(train)\n",
    "print('[{}] Finished to handle missing'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# Add raw text character and word length - this is stateless\n",
    "\n",
    "#item_descriptionの文字の数を加える\n",
    "#item_descriptionの単語の数を加える\n",
    "#カテゴリの段階数を加える\n",
    "#名前の文字の数を加える\n",
    "#名前の単語の数を加える\n",
    "add_character_and_word_lengths(data=train, app_series_man_=main_apply_series_man)\n",
    "print('[{}] Finished to add text lengths in train and test'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MEMORY USAGE for PID      14753 : 0.206\n",
      "[12.249096155166626] Finished to pre process text in train\n"
     ]
    }
   ],
   "source": [
    "#テキスト前処理（例えば単位の省略を直す、＜＞を加える、'veをhaveに直すなどしている）\n",
    "\n",
    "\n",
    "# Pre-process text features - this is stateless\n",
    "preprocess_text_features(df=train, app_series_man_=main_apply_series_man)\n",
    "cpuStats()\n",
    "print('[{}] Finished to pre process text in train'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.896924257278442] Finished hashing dataset\n",
      " MEMORY USAGE for PID      14753 : 0.206\n"
     ]
    }
   ],
   "source": [
    "# Get hashing space - this is stateless\n",
    "csr_name_trn = get_hashing_features(train, Hash_binary, start_time,\n",
    "                                    main_apply_series_man, main_apply_man, main_hash_man)\n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MEMORY USAGE for PID      14753 : 0.201\n",
      "Zeros found created\n",
      " MEMORY USAGE for PID      14753 : 0.264\n"
     ]
    }
   ],
   "source": [
    "# Reduce Hashing space\n",
    "trn_not_zeros = np.array((csr_name_trn.sum(axis=0) != 0))[0]\n",
    "cpuStats()\n",
    "print(\"Zeros found created\")\n",
    "csr_name_trn = csr_name_trn[:, trn_not_zeros]\n",
    "gc.collect()\n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300.88159823417664] Finished get brand for sgd in train\n",
      "[300.8888192176819] Finished get condition for sgd in train\n"
     ]
    }
   ],
   "source": [
    "# Get brand as dummies - this is stateful so we may need to do this again\n",
    "# Keep for now\n",
    "brand_man = OHEManager(feature_name=\"brand_name\")\n",
    "brand_man.add_factorized_feature_on_train(trn=train)\n",
    "csr_brand_trn = brand_man.get_feature_for_sgd_train(trn=train)\n",
    "print('[{}] Finished get brand for sgd in train'.format(time.time() - start_time))\n",
    "\n",
    "cond_man = OHEManager(feature_name=\"item_condition_id\")\n",
    "cond_man.add_factorized_feature_on_train(trn=train)\n",
    "csr_cond_trn = cond_man.get_feature_for_sgd_train(trn=train)\n",
    "print('[{}] Finished get condition for sgd in train'.format(time.time() - start_time))\n",
    "\n",
    "ship_man = OHEManager(feature_name=\"shipping\")\n",
    "ship_man.add_factorized_feature_on_train(trn=train)\n",
    "csr_ship_trn = ship_man.get_feature_for_sgd_train(trn=train)\n",
    "print('[{}] Finished get shipping for sgd in train'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_ship_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    3\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    3\n",
       "6    3\n",
       "7    3\n",
       "8    3\n",
       "9    3\n",
       "Name: item_condition_id, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"item_condition_id\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MEMORY USAGE for PID       9609 : 0.148\n",
      "Zeros found created\n",
      " MEMORY USAGE for PID       9609 : 0.211\n",
      "[3246.249939918518] Finished get brand for sgd in train\n",
      "[3246.256228208542] Finished get condition for sgd in train\n",
      "[3246.260267019272] Finished get shipping for sgd in train\n",
      "[3246.291116952896] Finished get category_name_0 for sgd in train\n",
      "[3246.2952580451965] Finished get category_name_1 for sgd in train\n",
      "[3246.298853158951] Finished get category_name_2 for sgd in train\n",
      "[3246.349440097809] Finished get category_name_0 for sgd in train\n",
      "[3246.356005191803] Finished get category_name_1 for sgd in train\n",
      "[3246.36302113533] Finished get category_name_2 for sgd in train\n",
      "[3246.419830083847] Finished get brand_cond for sgd in train\n",
      "csr matrices creation done for train MEMORY USAGE for PID       9609 : 0.211\n",
      "[3246.4502840042114] Finished get numerical features\n",
      "Cond       :  (100, 3)\n",
      "Ship       :  (100, 2)\n",
      "Brand      :  (100, 1)\n",
      "Brand Cond :  (100, 2)\n",
      "Num        :  (100, 22)\n",
      "Cat        :  (100, 16)\n",
      "Cat_Cond   :  (100, 10)\n",
      "Name       :  (100, 5007)\n",
      " MEMORY USAGE for PID       9609 : 0.211\n",
      "hstack done for train matrices with shape :  (100, 5063)\n",
      "Validation score for liblinear_fold_0 = 0.938416\n",
      "Validation score for ridge_fold_0 = 0.591342\n",
      "Validation score for liblinear_fold_1 = 1.084123\n",
      "Validation score for ridge_fold_1 = 0.573836\n",
      "Validation score for liblinear_fold_2 = 0.635775\n",
      "Validation score for ridge_fold_2 = 0.573870\n",
      "Validation score for liblinear_fold_3 = 1.289172\n",
      "Validation score for ridge_fold_3 = 0.719093\n",
      "Validation score for liblinear_fold_4 = 0.833693\n",
      "Validation score for ridge_fold_4 = 0.553176\n",
      "SGD L2 Fold  1 : 0.938416\n",
      "SGD L1 Fold  1 : 0.591342\n",
      "SGD L2 Fold  2 : 1.084123\n",
      "SGD L1 Fold  2 : 0.573836\n",
      "SGD L2 Fold  3 : 0.635775\n",
      "SGD L1 Fold  3 : 0.573870\n",
      "SGD L2 Fold  4 : 1.289172\n",
      "SGD L1 Fold  4 : 0.719093\n",
      "SGD L2 Fold  5 : 0.833693\n",
      "SGD L1 Fold  5 : 0.553176\n",
      " MEMORY USAGE for PID       9609 : 0.212\n",
      "[3246.688549041748] Finished delete training csr matrices\n",
      "==================================================\n",
      "Finished training ridge/liblinear\n",
      "==================================================\n",
      " MEMORY USAGE for PID       9609 : 0.212\n",
      "[ 3247.0] Finished create name features\n",
      "id                        0\n",
      "name                      0\n",
      "item_condition_id         0\n",
      "category_name             0\n",
      "brand_name                0\n",
      "price                     0\n",
      "shipping                  0\n",
      "item_description          0\n",
      "brand_name_median         0\n",
      "brand_name_mean           0\n",
      "brand_name_std            0\n",
      "brand_name_min            0\n",
      "brand_name_max            0\n",
      "category_name_median      0\n",
      "category_name_mean        0\n",
      "category_name_std         0\n",
      "category_name_min         0\n",
      "category_name_max         0\n",
      "desc_len                  0\n",
      "desc_word_len             0\n",
      "nb_categories             0\n",
      "name_len                  0\n",
      "name_word_len             0\n",
      "mix_cat_name              0\n",
      "fact_brand_name           0\n",
      "fact_item_condition_id    0\n",
      "fact_shipping             0\n",
      "category_name_0           0\n",
      "cat_cond_0                0\n",
      "category_name_1           0\n",
      "cat_cond_1                0\n",
      "category_name_2           0\n",
      "cat_cond_2                0\n",
      "fact_category_name_0      0\n",
      "fact_category_name_1      0\n",
      "fact_category_name_2      0\n",
      "fact_cat_cond_0           0\n",
      "fact_cat_cond_1           0\n",
      "fact_cat_cond_2           0\n",
      "brd_cond                  0\n",
      "fact_brd_cond             0\n",
      "sgd_liblinear             0\n",
      "sgd_ridge                 0\n",
      "liblinear_ridge           0\n",
      "fact_name_0               0\n",
      "fact_name_1               0\n",
      "fact_name_2               0\n",
      "fact_name_3               0\n",
      "fact_name_4               0\n",
      "fact_name_5               0\n",
      "dtype: int64\n",
      "[3246.9757850170135] Finished get train numerical data for Light GBM\n",
      "NAN IN NUM :  0\n",
      " MEMORY USAGE for PID       9609 : 0.212\n",
      "[3247.020215034485] Finished deleting train data\n",
      " MEMORY USAGE for PID       9609 : 0.212\n",
      "Feature reduction done, X_name shape  (100, 0)  after col pruning\n",
      "Clipping computed\n",
      "X_name reduced\n",
      " MEMORY USAGE for PID       9609 : 0.229\n",
      "[3247.6233489513397] TFIDF WordBatch features done for name\n",
      "After csr_lgb_trn stacking MEMORY USAGE for PID       9609 : 0.229\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "b'Cannot construct Dataset since there are not useful features.                 It should be at least two unique rows.                 If the num_row (num_data) is small, you can set min_data=1 and min_data_in_bin=1 to fix this.                 Otherwise please make sure you are using the right dataset.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5204ce7e0d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb1_rounds\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# was 700, 400 comes from OOF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             verbose_eval=100)\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Train lgb l2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         lgb_l2 = lgb.train(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;34m\"\"\"construct booster\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m                 ctypes.byref(self.handle)))\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    820\u001b[0m                                 \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[1;32m    823\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m(self, data, label, max_bin, reference, weight, group, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 ctypes.byref(self.handle)))\n\u001b[1;32m    676\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init_from_csr\u001b[0;34m(self, csr, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mref_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             ctypes.byref(self.handle)))\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLightGBMError\u001b[0m: b'Cannot construct Dataset since there are not useful features.                 It should be at least two unique rows.                 If the num_row (num_data) is small, you can set min_data=1 and min_data_in_bin=1 to fix this.                 Otherwise please make sure you are using the right dataset.'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "add_categories_and_mix_with_condition(df=train)\n",
    "\n",
    "# Get categories as dummies\n",
    "cat0_man = OHEManager(feature_name=\"category_name_0\")\n",
    "cat0_man.add_factorized_feature_on_train(trn=train)\n",
    "csr_cat0_trn = cat0_man.get_feature_for_sgd_train(trn=train)\n",
    "print('[{}] Finished get category_name_0 for sgd in train'.format(time.time() - start_time))\n",
    "\n",
    "cat1_man = OHEManager(feature_name=\"category_name_1\")\n",
    "cat1_man.add_factorized_feature_on_train(trn=train)\n",
    "csr_cat1_trn = cat1_man.get_feature_for_sgd_train(trn=train)\n",
    "print('[{}] Finished get category_name_1 for sgd in train'.format(time.time() - start_time))\n",
    "\n",
    "cat2_man = OHEManager(feature_name=\"category_name_2\")\n",
    "cat2_man.add_factorized_feature_on_train(trn=train)\n",
    "csr_cat2_trn = cat2_man.get_feature_for_sgd_train(trn=train)\n",
    "print('[{}] Finished get category_name_2 for sgd in train'.format(time.time() - start_time))\n",
    "\n",
    "csr_cat_trn = hstack((csr_cat0_trn, csr_cat1_trn, csr_cat2_trn)).tocsr()\n",
    "del csr_cat0_trn, csr_cat1_trn, csr_cat2_trn\n",
    "gc.collect()\n",
    "\n",
    "# Get categories + condition as dummies\n",
    "cat0_cond_man = OHEManager(feature_name=\"cat_cond_0\")\n",
    "cat0_cond_man.add_factorized_feature_on_train(trn=train)\n",
    "csr_cat0_cond_trn = cat0_cond_man.get_feature_for_sgd_train(trn=train)\n",
    "print('[{}] Finished get category_name_0 for sgd in train'.format(time.time() - start_time))\n",
    "\n",
    "cat1_cond_man = OHEManager(feature_name=\"cat_cond_1\")\n",
    "cat1_cond_man.add_factorized_feature_on_train(trn=train)\n",
    "csr_cat1_cond_trn = cat1_cond_man.get_feature_for_sgd_train(trn=train)\n",
    "print('[{}] Finished get category_name_1 for sgd in train'.format(time.time() - start_time))\n",
    "\n",
    "cat2_cond_man = OHEManager(feature_name=\"cat_cond_2\")\n",
    "cat2_cond_man.add_factorized_feature_on_train(trn=train)\n",
    "csr_cat2_cond_trn = cat2_cond_man.get_feature_for_sgd_train(trn=train)\n",
    "print('[{}] Finished get category_name_2 for sgd in train'.format(time.time() - start_time))\n",
    "\n",
    "csr_cat_cond_trn = hstack((csr_cat0_cond_trn, csr_cat1_cond_trn, csr_cat2_cond_trn)).tocsr()\n",
    "del csr_cat0_cond_trn, csr_cat1_cond_trn, csr_cat2_cond_trn\n",
    "gc.collect()\n",
    "\n",
    "train[\"brd_cond\"] = train[\"brand_name\"].astype(str) + \"|\" + train[\"item_condition_id\"].astype(str)\n",
    "brd_cond_man = OHEManager(feature_name=\"brd_cond\")\n",
    "brd_cond_man.add_factorized_feature_on_train(trn=train)\n",
    "csr_brd_cond_trn = brd_cond_man.get_feature_for_sgd_train(trn=train)\n",
    "print('[{}] Finished get brand_cond for sgd in train'.format(time.time() - start_time))\n",
    "\n",
    "cpuStats(\"csr matrices creation done for train\")\n",
    "\n",
    "csr_num_trn, trn_minmax_skl = get_numerical_features(\n",
    "    train,\n",
    "    numericals=get_numerical_features_for_sgd(train),\n",
    "    gaussian=True,\n",
    "    rank=False\n",
    ")\n",
    "print('[{}] Finished get numerical features'.format(time.time() - start_time))\n",
    "\n",
    "# Build csr matrices for SGD fitting and predictions\n",
    "print(\"Cond       : \", csr_cond_trn.shape)\n",
    "print(\"Ship       : \", csr_ship_trn.shape)\n",
    "print(\"Brand      : \", csr_brand_trn.shape)\n",
    "print(\"Brand Cond : \", csr_brd_cond_trn.shape)\n",
    "print(\"Num        : \", csr_num_trn.shape)\n",
    "print(\"Cat        : \", csr_cat_trn.shape)\n",
    "print(\"Cat_Cond   : \", csr_cat_cond_trn.shape)\n",
    "print(\"Name       : \", csr_name_trn.shape)\n",
    "csr_ridge_trn = hstack((\n",
    "    csr_cond_trn,\n",
    "    csr_ship_trn,\n",
    "    csr_brand_trn,\n",
    "    csr_brd_cond_trn,\n",
    "    csr_num_trn,\n",
    "    csr_cat_trn,\n",
    "    csr_cat_cond_trn,\n",
    "    csr_name_trn,\n",
    ")).tocsr()\n",
    "\n",
    "del csr_num_trn\n",
    "del csr_brand_trn\n",
    "del csr_cat_cond_trn\n",
    "del csr_cat_trn\n",
    "gc.collect()\n",
    "cpuStats()\n",
    "print(\"hstack done for train matrices with shape : \", csr_ridge_trn.shape)\n",
    "\n",
    "# Fit SGD models\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "models_list = fit_sgd_models(csr_ridge_trn, folds, y)\n",
    "\n",
    "# Predict OOF data\n",
    "oof_liblinear_preds, oof_ridge_preds = get_sgd_oof_predictions(csr_ridge_trn, folds, models_list, y)\n",
    "\n",
    "del csr_ridge_trn\n",
    "gc.collect()\n",
    "cpuStats()\n",
    "print('[{}] Finished delete training csr matrices'.format(time.time() - start_time))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Finished training ridge/liblinear\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Add OOF predictions to train data\n",
    "train[\"sgd_liblinear\"] = np.expm1(oof_liblinear_preds)\n",
    "train[\"sgd_ridge\"] = np.expm1(oof_ridge_preds)\n",
    "train[\"liblinear_ridge\"] = .50 * np.expm1(oof_liblinear_preds) + .50 * np.expm1(oof_ridge_preds)\n",
    "\n",
    "# add name features\n",
    "name_indexers = add_name_features_for_train(df=train)\n",
    "cpuStats()\n",
    "print('[{0:7.1f}] Finished create name features'.format(time.time() - start_time))\n",
    "print(train.isnull().sum())\n",
    "csr_num_trn = get_numerical_features(\n",
    "    train,\n",
    "    numericals=get_numerical_features_for_lgb(train),\n",
    "    gaussian=False,\n",
    "    rank=True\n",
    ")\n",
    "print('[{}] Finished get train numerical data for Light GBM'.format(time.time() - start_time))\n",
    "\n",
    "print(\"NAN IN NUM : \", np.isnan(np.array((csr_num_trn.sum(axis=0)))[0]).sum())\n",
    "\n",
    "name_trn = train[[\"name\"]].copy()\n",
    "del train\n",
    "gc.collect()\n",
    "cpuStats()\n",
    "print('[{}] Finished deleting train data'.format(time.time() - start_time))\n",
    "\n",
    "indices_low = np.arange(csr_name_trn.shape[1])\n",
    "util_cols_low_trn = np.array((csr_name_trn.sum(axis=0) >= 500))[0]\n",
    "csr_name_trn = csr_name_trn[:, indices_low[util_cols_low_trn]]\n",
    "\n",
    "indices_high = np.arange(csr_name_trn.shape[1])\n",
    "util_cols_high_trn = np.array((csr_name_trn.sum(axis=0) < 200000))[0]\n",
    "csr_name_trn = csr_name_trn[:, indices_high[util_cols_high_trn]]\n",
    "\n",
    "gc.collect()\n",
    "cpuStats()\n",
    "print(\"Feature reduction done, X_name shape \", csr_name_trn.shape, \" after col pruning\")\n",
    "\n",
    "csr_tfidfname_trn, wordbatch_tfidf, clipping = get_tfidf_features_for_train(name_trn, hash_man_=main_hash_man)\n",
    "cpuStats()\n",
    "print('[{}] TFIDF WordBatch features done for name'.format(time.time() - start_time))\n",
    "\n",
    "csr_lgb_trn = hstack((\n",
    "    csr_tfidfname_trn,\n",
    "    csr_name_trn,\n",
    "    csr_num_trn,\n",
    ")).tocsr()\n",
    "\n",
    "del csr_name_trn, csr_num_trn, csr_tfidfname_trn\n",
    "gc.collect()\n",
    "cpuStats(\"After csr_lgb_trn stacking\")\n",
    "\n",
    "# Create parameters\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    'metric': {'rmse'},\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"verbosity\": 0,\n",
    "    \"num_threads\": 4,\n",
    "    \"bagging_fraction\": 0.78,\n",
    "    \"feature_fraction\": 0.76,\n",
    "    \"learning_rate\": 0.4,\n",
    "    \"min_child_weight\": 197,\n",
    "    \"min_data_in_leaf\": 197,\n",
    "    \"num_leaves\": 103,\n",
    "}\n",
    "\n",
    "params_l2 = {\n",
    "    'learning_rate': 0.4,\n",
    "    'application': 'regression_l2',\n",
    "    'max_depth': 4,\n",
    "    'num_leaves': 70,\n",
    "    'verbosity': -1,\n",
    "    \"min_split_gain\": 0,\n",
    "    'lambda_l1': 4,\n",
    "    'subsample': 1,\n",
    "    \"bagging_freq\": 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'metric': 'RMSE',\n",
    "    'nthread': 4\n",
    "}\n",
    "\n",
    "lgb1_rounds = 500\n",
    "lgb2_rounds = 4000\n",
    "\n",
    "if ensemble:\n",
    "    # Run LGB 1 and LGB 2\n",
    "    # Reuse folds defined for ridge/sgd\n",
    "    # to avoid overfitting\n",
    "    if mode in [PROD_OOF, VALID_TRN, STAGE2_OOF, FAST_VALID]:\n",
    "        for fold_n, (trn_idx, val_idx) in enumerate(folds.split(csr_lgb_trn)):\n",
    "            d_train = lgb.Dataset(csr_lgb_trn[trn_idx], label=y[trn_idx])  # , max_bin=8192)\n",
    "            d_valid = lgb.Dataset(csr_lgb_trn[val_idx], label=y[val_idx])  # , max_bin=8192)\n",
    "            watchlist = [d_train, d_valid]\n",
    "            cpuStats()\n",
    "            # Train lgb l1\n",
    "            lgb_l1 = lgb.train(\n",
    "                params=params,\n",
    "                train_set=d_train,\n",
    "                num_boost_round=lgb1_rounds,\n",
    "                valid_sets=watchlist,\n",
    "                verbose_eval=100)\n",
    "            # Train lgb l2\n",
    "            lgb_l2 = lgb.train(\n",
    "                params=params_l2,\n",
    "                train_set=d_train,\n",
    "                num_boost_round=lgb2_rounds,\n",
    "                valid_sets=watchlist,\n",
    "                verbose_eval=500)\n",
    "\n",
    "            break\n",
    "        # Check OOF score of ensemble ?\n",
    "        oof_l1 = lgb_l1.predict(csr_lgb_trn[val_idx])\n",
    "        oof_l2 = lgb_l2.predict(csr_lgb_trn[val_idx])\n",
    "\n",
    "        print(\"OOF error L1   : %.6f \"\n",
    "              % mean_squared_error(y[val_idx], oof_l1) ** .5)\n",
    "        print(\"OOF error L2   : %.6f \"\n",
    "              % mean_squared_error(y[val_idx], oof_l2) ** .5)\n",
    "        print(\"OOF error Mix1 : %.6f \"\n",
    "              % mean_squared_error(y[val_idx], oof_l1 * .3 + oof_l2 * .7) ** .5)\n",
    "        oof_preds = np.expm1(oof_l2) * .7 + np.expm1(oof_l1) * .3\n",
    "        print(\"OOF error Mix2 : %.6f \"\n",
    "              % mean_squared_error(y[val_idx], np.log1p(oof_preds)) ** .5)\n",
    "    else:\n",
    "        d_train = lgb.Dataset(csr_lgb_trn, label=y)\n",
    "        watchlist = [d_train]\n",
    "        # Train lgb l1\n",
    "        lgb_l1 = lgb.train(\n",
    "            params=params,\n",
    "            train_set=d_train,\n",
    "            num_boost_round=lgb1_rounds,  # was 700, 400 comes from OOF\n",
    "            valid_sets=watchlist,\n",
    "            verbose_eval=100)\n",
    "        # Train lgb l2\n",
    "        lgb_l2 = lgb.train(\n",
    "            params=params_l2,\n",
    "            train_set=d_train,\n",
    "            num_boost_round=lgb2_rounds,  # Beware this is not the same as in OOF mode\n",
    "            valid_sets=watchlist,\n",
    "            verbose_eval=500)\n",
    "\n",
    "else:\n",
    "    # Only LGB L2 is trained\n",
    "    if mode in [PROD_OOF, VALID_TRN, STAGE2_OOF, FAST_VALID]:\n",
    "        for fold_n, (trn_idx, val_idx) in enumerate(folds.split(csr_lgb_trn)):\n",
    "            d_train = lgb.Dataset(csr_lgb_trn[trn_idx], label=y[trn_idx])  # , max_bin=8192)\n",
    "            d_valid = lgb.Dataset(csr_lgb_trn[val_idx], label=y[val_idx])  # , max_bin=8192)\n",
    "            watchlist = [d_train, d_valid]\n",
    "            cpuStats()\n",
    "            # Train lgb l2\n",
    "            lgb_l2 = lgb.train(\n",
    "                params=params_l2,\n",
    "                train_set=d_train,\n",
    "                num_boost_round=lgb2_rounds,\n",
    "                valid_sets=watchlist,\n",
    "                verbose_eval=500)\n",
    "\n",
    "            break\n",
    "            # Check OOF score of ensemble ?\n",
    "\n",
    "    else:\n",
    "        d_train = lgb.Dataset(csr_lgb_trn, label=y)\n",
    "        watchlist = [d_train]\n",
    "        # Train lgb l2\n",
    "        lgb_l2 = lgb.train(\n",
    "            params=params_l2,\n",
    "            train_set=d_train,\n",
    "            num_boost_round=lgb2_rounds,  # Beware this is not the same as in OOF mode\n",
    "            valid_sets=watchlist,\n",
    "            verbose_eval=500)\n",
    "\n",
    "del csr_lgb_trn\n",
    "gc.collect()\n",
    "cpuStats()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING PART HAS NOW COMPLETE, STARTING PREDICTION PART\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Read test dataset\n",
    "test2 = data_man.get_test_data()\n",
    "\n",
    "# test2 = corrupt(test2)\n",
    "\n",
    "print('[{}] Finished to load test data'.format(time.time() - start_time), ' test shape: ', test2.shape)\n",
    "\n",
    "y_test2 = None\n",
    "if \"price\" in test2:\n",
    "    y_test2 = np.log1p(test2[\"price\"].values)\n",
    "submission = test2[[\"id\"]].copy()\n",
    "\n",
    "# Building and predicting using batches\n",
    "batch_size = 100000\n",
    "submission_preds = np.zeros(len(test2))\n",
    "\n",
    "# Go through batches\n",
    "for i_b in range(math.ceil(len(test2) / batch_size)):\n",
    "    print(\"Building and predicting for batch %4d\" % i_b)\n",
    "    s_b, e_b = i_b * batch_size, (i_b + 1) * batch_size\n",
    "\n",
    "    test = test2.iloc[s_b: e_b].copy()\n",
    "\n",
    "    y_test = None\n",
    "    if \"price\" in test:\n",
    "        y_test = np.log1p(test[\"price\"].values)\n",
    "\n",
    "    # Create price statistics by brand\n",
    "    # brand price mappers should contain mapping for mean, std,\n",
    "    test = add_price_statistics_on_test(sub=test, stats=brand_price_mappers, target=y, feature=\"brand_name\")\n",
    "    test = add_price_statistics_on_test(sub=test, stats=cat_price_mappers, target=y, feature=\"category_name\")\n",
    "\n",
    "    # Replace NaN\n",
    "    handle_missing_inplace(test)\n",
    "    # print('[{}] Finished to handle missing'.format(time.time() - start_time))\n",
    "\n",
    "    # Now create test set matrices, the one we miss is the hashing matrix\n",
    "    add_character_and_word_lengths(data=test, app_series_man_=main_apply_series_man)\n",
    "    # cpuStats()\n",
    "    preprocess_text_features(df=test, app_series_man_=main_apply_series_man)\n",
    "    # cpuStats()\n",
    "\n",
    "    csr_name_sub2 = get_hashing_features(test, Hash_binary, start_time,\n",
    "                                         main_apply_series_man, main_apply_man, main_hash_man)\n",
    "    # Reduce hashing space like train\n",
    "    csr_name_sub = csr_name_sub2[:, trn_not_zeros]\n",
    "    del csr_name_sub2\n",
    "    gc.collect()\n",
    "    # cpuStats()\n",
    "\n",
    "    # Get brand as dummies - this is stateful so we may need to do this again\n",
    "    # Keep for now\n",
    "    brand_man.add_factorized_feature_on_test(sub=test)\n",
    "    csr_brand_sub = brand_man.get_feature_for_sgd_test(sub=test)\n",
    "    # print('[{}] Finished get brand for sgd in test'.format(time.time() - start_time))\n",
    "\n",
    "    cond_man.add_factorized_feature_on_test(sub=test)\n",
    "    csr_cond_sub = cond_man.get_feature_for_sgd_test(sub=test)\n",
    "    # print('[{}] Finished get condition for sgd in test'.format(time.time() - start_time))\n",
    "\n",
    "    ship_man.add_factorized_feature_on_test(sub=test)\n",
    "    csr_ship_sub = ship_man.get_feature_for_sgd_test(sub=test)\n",
    "    # print('[{}] Finished get shipping for sgd in test'.format(time.time() - start_time))\n",
    "\n",
    "    add_categories_and_mix_with_condition(df=test)\n",
    "\n",
    "    # Get categories as dummies\n",
    "    cat0_man.add_factorized_feature_on_test(sub=test)\n",
    "    csr_cat0_sub = cat0_man.get_feature_for_sgd_test(sub=test)\n",
    "    # print('[{}] Finished get category_name_0 for sgd in test'.format(time.time() - start_time))\n",
    "\n",
    "    cat1_man.add_factorized_feature_on_test(sub=test)\n",
    "    csr_cat1_sub = cat1_man.get_feature_for_sgd_test(sub=test)\n",
    "    # print('[{}] Finished get category_name_1 for sgd in test'.format(time.time() - start_time))\n",
    "\n",
    "    cat2_man.add_factorized_feature_on_test(sub=test)\n",
    "    csr_cat2_sub = cat2_man.get_feature_for_sgd_test(sub=test)\n",
    "    # print('[{}] Finished get category_name_2 for sgd in test'.format(time.time() - start_time))\n",
    "\n",
    "    csr_cat_sub = hstack((csr_cat0_sub, csr_cat1_sub, csr_cat2_sub)).tocsr()\n",
    "    del csr_cat0_sub, csr_cat1_sub, csr_cat2_sub\n",
    "    gc.collect()\n",
    "\n",
    "    # Get categories + condition as dummies\n",
    "    cat0_cond_man.add_factorized_feature_on_test(sub=test)\n",
    "    csr_cat0_cond_sub = cat0_cond_man.get_feature_for_sgd_test(sub=test)\n",
    "    # print('[{}] Finished get category_name_0 for sgd in test'.format(time.time() - start_time))\n",
    "\n",
    "    cat1_cond_man.add_factorized_feature_on_test(sub=test)\n",
    "    csr_cat1_cond_sub = cat1_cond_man.get_feature_for_sgd_test(sub=test)\n",
    "    # print('[{}] Finished get category_name_1 for sgd in test'.format(time.time() - start_time))\n",
    "\n",
    "    cat2_cond_man.add_factorized_feature_on_test(sub=test)\n",
    "    csr_cat2_cond_sub = cat2_cond_man.get_feature_for_sgd_test(sub=test)\n",
    "    # print('[{}] Finished get category_name_2 for sgd in test'.format(time.time() - start_time))\n",
    "\n",
    "    csr_cat_cond_sub = hstack((csr_cat0_cond_sub, csr_cat1_cond_sub, csr_cat2_cond_sub)).tocsr()\n",
    "    del csr_cat0_cond_sub, csr_cat1_cond_sub, csr_cat2_cond_sub\n",
    "    gc.collect()\n",
    "\n",
    "    test[\"brd_cond\"] = test[\"brand_name\"].astype(str) + \"|\" + test[\"item_condition_id\"].astype(str)\n",
    "    brd_cond_man.add_factorized_feature_on_test(sub=test)\n",
    "    csr_brd_cond_sub = brd_cond_man.get_feature_for_sgd_test(sub=test)\n",
    "    # print('[{}] Finished get brand_cond for sgd in train'.format(time.time() - start_time))\n",
    "\n",
    "    csr_num_sub, _ = get_numerical_features(\n",
    "        test,\n",
    "        numericals=get_numerical_features_for_sgd(test),\n",
    "        gaussian=True,\n",
    "        minmax_skl=trn_minmax_skl,\n",
    "        rank=False\n",
    "    )\n",
    "    # print('[{}] Finished getting numerical features for test'.format(time.time() - start_time))\n",
    "\n",
    "    # for i_b in range(math.ceil(len(test) / batch_size)):\n",
    "    # s_b, e_b = i_b * batch_size, (i_b + 1) * batch_size\n",
    "\n",
    "    csr_ridge_sub = hstack((\n",
    "        csr_cond_sub,\n",
    "        csr_ship_sub,\n",
    "        csr_brand_sub,\n",
    "        csr_brd_cond_sub,\n",
    "        csr_num_sub,\n",
    "        csr_cat_sub,\n",
    "        csr_cat_cond_sub,\n",
    "        csr_name_sub\n",
    "    )).tocsr()\n",
    "\n",
    "    # print('[{}] Finished creating batch for test'.format(time.time() - start_time))\n",
    "    sub_liblinear_preds = np.empty(len(test))\n",
    "    sub_ridge_preds = np.empty(len(test))\n",
    "    # Get sgd predictions for test\n",
    "    if y_test is not None:\n",
    "        sub_liblinear_preds, sub_ridge_preds = \\\n",
    "            get_sgd_test_predictions(csr_ridge_sub,\n",
    "                                     folds,\n",
    "                                     models_list,\n",
    "                                     y_test)\n",
    "    else:\n",
    "        sub_liblinear_preds, sub_ridge_preds = \\\n",
    "            get_sgd_test_predictions(csr_ridge_sub, folds, models_list)\n",
    "\n",
    "    # Delete csr matrices we do not need anymore\n",
    "    del csr_ridge_sub\n",
    "    gc.collect()\n",
    "\n",
    "    del csr_cond_sub, csr_ship_sub\n",
    "    del csr_num_sub\n",
    "    del csr_brand_sub\n",
    "    del csr_cat_cond_sub\n",
    "    del csr_cat_sub\n",
    "    gc.collect()\n",
    "\n",
    "    # Now make sure all went well if possible\n",
    "    print(\"Done predicting Ridge/LinearSVR\")\n",
    "    if y_test is not None:\n",
    "        print(\"Test liblinear error : \", mean_squared_error(y_test, sub_liblinear_preds) ** .5)\n",
    "        print(\"ridge error : \", mean_squared_error(y_test, sub_ridge_preds) ** .5)\n",
    "\n",
    "    # cpuStats()\n",
    "    # print('[{}] Finished delete test csr matrices'.format(time.time() - start_time))\n",
    "\n",
    "    test[\"sgd_liblinear\"] = np.expm1(sub_liblinear_preds)\n",
    "    test[\"sgd_ridge\"] = np.expm1(sub_ridge_preds)\n",
    "    test[\"liblinear_ridge\"] = .50 * np.expm1(sub_liblinear_preds) + .50 * np.expm1(sub_ridge_preds)\n",
    "\n",
    "    # Reduce csr_name_sub\n",
    "    csr_name_sub = csr_name_sub[:, indices_low[util_cols_low_trn]]\n",
    "    csr_name_sub = csr_name_sub[:, indices_high[util_cols_high_trn]]\n",
    "    gc.collect()\n",
    "    # cpuStats()\n",
    "    # print(\"Feature reduction done, X_name shape \", csr_name_sub.shape, \" after col pruning\")\n",
    "\n",
    "    add_name_features_for_test(df=test, indexers=name_indexers)\n",
    "\n",
    "    # print(test.columns)\n",
    "\n",
    "    # cpuStats()\n",
    "    # print('[{0:7.1f}] Finished create name features'.format(time.time() - start_time))\n",
    "\n",
    "    csr_num_sub = get_numerical_features(test, numericals=get_numerical_features_for_lgb(test), gaussian=False,\n",
    "                                         rank=True)\n",
    "    # cpuStats()\n",
    "    # print('[{}] Finished get test numerical data for Light GBM'.format(time.time() - start_time))\n",
    "\n",
    "    name_sub = test[[\"name\"]].copy()\n",
    "\n",
    "    # We do not need test anymore\n",
    "    del test\n",
    "    gc.collect()\n",
    "    # cpuStats()\n",
    "    # print('[{}] Finished deleting test data'.format(time.time() - start_time))\n",
    "\n",
    "    # Compute test matrices\n",
    "    csr_tfidfname_sub = get_tfidf_features_for_test(name_sub, wordbatch_tfidf, clipping, hash_man_=main_hash_man)\n",
    "    # cpuStats()\n",
    "    # print('[{}] Finished get test tfidf data for Light GBM'.format(time.time() - start_time))\n",
    "\n",
    "    if ensemble:\n",
    "\n",
    "        predsL1 = np.empty(len(submission))\n",
    "        predsL2 = np.empty(len(submission))\n",
    "\n",
    "        csr_lgb_sub = hstack((\n",
    "            csr_tfidfname_sub,\n",
    "            csr_name_sub,\n",
    "            csr_num_sub,\n",
    "        )).tocsr()\n",
    "\n",
    "        # print('[{}] Finished creating batch for test'.format(time.time() - start_time))\n",
    "\n",
    "        # Get predictions for test\n",
    "        predsL1 = lgb_l1.predict(csr_lgb_sub)\n",
    "        predsL2 = lgb_l2.predict(csr_lgb_sub)\n",
    "\n",
    "        # Delete csr matrices we do not need anymore\n",
    "        del csr_lgb_sub\n",
    "        gc.collect()\n",
    "\n",
    "        del csr_num_sub, csr_name_sub, csr_tfidfname_sub\n",
    "        gc.collect()\n",
    "        print(\"Finished predicting LGB1 and LGB2\")\n",
    "        if y_test is not None:\n",
    "            preds = np.expm1(predsL2) * .7 + np.expm1(predsL1) * .3\n",
    "            print(\"Test error L1   : %.6f \"\n",
    "                  % mean_squared_error(y_test, predsL1) ** .5)\n",
    "            print(\"Test error L2   : %.6f \"\n",
    "                  % mean_squared_error(y_test, predsL2) ** .5)\n",
    "            print(\"Test error Mix  : %.6f \"\n",
    "                  % mean_squared_error(y_test, np.log1p(preds)) ** .5)\n",
    "\n",
    "        submission_preds[s_b: e_b] = np.expm1(predsL2) * .65 + np.expm1(predsL1) * .35\n",
    "\n",
    "    else:\n",
    "\n",
    "        predsL2 = np.empty(len(submission))\n",
    "\n",
    "        csr_lgb_sub = hstack((\n",
    "            csr_tfidfname_sub,\n",
    "            csr_name_sub,\n",
    "            csr_num_sub,\n",
    "        )).tocsr()\n",
    "\n",
    "        # print('[{}] Finished creating batch for test'.format(time.time() - start_time))\n",
    "\n",
    "        # Get predictions for test\n",
    "        predsL2[s_b: e_b] = lgb_l2.predict(csr_lgb_sub)\n",
    "\n",
    "        # Delete csr matrices we do not need anymore\n",
    "        del csr_lgb_sub\n",
    "        gc.collect()\n",
    "\n",
    "        del csr_num_sub, csr_name_sub, csr_tfidfname_sub\n",
    "        gc.collect()\n",
    "        print(\"Finished predicting LGB2\")\n",
    "        if y_test is not None:\n",
    "            print(\"Test error L2  : %.6f \" % mean_squared_error(y_test, predsL2) ** .5)\n",
    "\n",
    "        submission_preds[s_b: e_b] = np.expm1(predsL2)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PREDICTION PART FINISHED\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if y_test2 is not None:\n",
    "    print(\"Full test error : %.6f \" % mean_squared_error(y_test2, np.log1p(submission_preds)) ** .5)\n",
    "else:\n",
    "    # Write submission\n",
    "    submission[\"price\"] = np.clip(submission_preds,0,100000)\n",
    "    submission[\"test_id\"] = test2[\"id\"]\n",
    "    submission[['test_id', 'price']].to_csv(\"ridge_lgbm_around_the_world_submission.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
